<!DOCTYPE html>
<html lang=" en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“ | Tianxinâ€™s Blog</title>
<meta name="generator" content="Jekyll v4.2.0">
<meta property="og:title" content="æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“">
<meta name="author" content="Tianxin Xie">
<meta property="og:locale" content="en_US">
<meta name="description" content="åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ€§èƒ½ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒä¸€ç¯‡ reviewï¼Œæ€»ç»“äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç¿»è¯‘è‡ªè¯¥ç¯‡ reviewï¼Œåšäº†å¢åˆ æ”¹ï¼Œå¹¶åŠ ä¸Šäº†éƒ¨åˆ†è‡ªå·±çš„ç†è§£ï¼Œä»¥åŠé¢å¤–çš„å›¾ç¤ºã€è§†é¢‘å’Œé˜…è¯»ææ–™ã€‚">
<meta property="og:description" content="åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ€§èƒ½ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒä¸€ç¯‡ reviewï¼Œæ€»ç»“äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç¿»è¯‘è‡ªè¯¥ç¯‡ reviewï¼Œåšäº†å¢åˆ æ”¹ï¼Œå¹¶åŠ ä¸Šäº†éƒ¨åˆ†è‡ªå·±çš„ç†è§£ï¼Œä»¥åŠé¢å¤–çš„å›¾ç¤ºã€è§†é¢‘å’Œé˜…è¯»ææ–™ã€‚">
<link rel="canonical" href="https://xietx1995.github.io/2021/optimization">
<meta property="og:url" content="https://xietx1995.github.io/2021/optimization">
<meta property="og:site_name" content="Tianxinâ€™s Blog">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2021-07-04T00:00:00+08:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“">
<script type="application/ld+json">
{"description":"åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ€§èƒ½ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒä¸€ç¯‡ reviewï¼Œæ€»ç»“äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç¿»è¯‘è‡ªè¯¥ç¯‡ reviewï¼Œåšäº†å¢åˆ æ”¹ï¼Œå¹¶åŠ ä¸Šäº†éƒ¨åˆ†è‡ªå·±çš„ç†è§£ï¼Œä»¥åŠé¢å¤–çš„å›¾ç¤ºã€è§†é¢‘å’Œé˜…è¯»ææ–™ã€‚","headline":"æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“","dateModified":"2021-07-04T00:00:00+08:00","datePublished":"2021-07-04T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://xietx1995.github.io/2021/optimization"},"url":"https://xietx1995.github.io/2021/optimization","author":{"@type":"Person","name":"Tianxin Xie"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="shortcut icon" href="/assets/images/favicon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <script src="/assets/js/main.js"></script><link type="application/atom+xml" rel="alternate" href="https://xietx1995.github.io/feed.xml" title="Tianxin's Blog">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js"></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>



























































































































<header class="site-header " role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/">
  <img class="site-favicon" title="Tianxin's Blog" src="/assets/images/favicon.png" onerror="this.style.display='none'">
</a></span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">
            
            <a class="page-link" href="/">HOME</a><a class="page-link" href="/categories.html">CATEGORIES</a><a class="page-link" href="/archives.html">ARCHIVES</a><a class="page-link" href="/tags.html">TAGS</a><a class="page-link" href="/about.html">ABOUT</a>

            <!--<a class="page-link" href="/about.html">ABOUT</a><a class="page-link" href="/archives.html">ARCHIVES</a><a class="page-link" href="/categories.html">CATEGORIES</a><a class="page-link" href="/">HOME</a><a class="page-link" href="/tags.html">TAGS</a>-->




</div>
        </nav>
</div>
  </div>
</header>

<script>
  (function() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  })();
</script>
















































































































































<script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<!--<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('auto' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
-->

  <main class="page-content" aria-label="Content">
    <div class="wrapper">
      <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“</h1>
  <h2 class="post-subtitle"></h2>

  <p class="post-meta">
    <time class="dt-published" datetime="2021-07-04T00:00:00+08:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Jul 04, 2021
    </time>

    
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 13 mins</span>
  </p>
<div class="post-tags"><a class="post-tag" href="/tags.html#ai">#ai</a></div></header>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <p>åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ€§èƒ½ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒä¸€ç¯‡ <a href="https://ruder.io/optimizing-gradient-descent/">review</a>ï¼Œæ€»ç»“äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç¿»è¯‘è‡ªè¯¥ç¯‡ <a href="https://ruder.io/optimizing-gradient-descent/">review</a>ï¼Œåšäº†å¢åˆ æ”¹ï¼Œå¹¶åŠ ä¸Šäº†éƒ¨åˆ†è‡ªå·±çš„ç†è§£ï¼Œä»¥åŠé¢å¤–çš„å›¾ç¤ºã€è§†é¢‘å’Œé˜…è¯»ææ–™ã€‚</p>

<blockquote>
  <p>This blog post aims at providing you with intuitions towards the behaviour of different algorithms for optimizing gradient descent that will help you put them to use. We are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training. Subsequently, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules. We will also take a short look at algorithms and architectures to optimize gradient descent in a parallel and distributed setting. Finally, we will consider additional strategies that are helpful for optimizing gradient descent.</p>

  <p>â€‹                                                                                              â€“  â€œAn overview of gradient descent optimization algorithmsâ€</p>
</blockquote>

<h2 id="1-æ¢¯åº¦ä¸‹é™çš„å˜ç§">1 æ¢¯åº¦ä¸‹é™çš„å˜ç§</h2>

<p>è¿™é‡Œä»‹ç»ä¸‰ç§æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼š</p>

<ul>
  <li>Batch Gradient Descent</li>
  <li>Stochastic Gradient Descent</li>
  <li>Mini-batch gradient descent</li>
</ul>

<p>è¿™ä¸‰ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•çš„å”¯ä¸€åŒºåˆ«å°±æ˜¯æ¯ä¸ª step ä½¿ç”¨çš„æ•°æ®é‡ä¸åŒã€‚</p>

<h3 id="11-batch-gradient-descent">1.1 Batch Gradient Descent</h3>

<p>è¿™æ˜¯æœ€åŸå§‹çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œç¿»è¯‘è¿‡æ¥å°±æ˜¯â€æ‰¹é‡æ¢¯åº¦ä¸‹é™â€œï¼Œæ¯ä¸ª step ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†ã€‚</p>

\[\theta = \theta - \eta\cdot\triangledown_{\theta} J(\theta)\]

<p>è¿™ä¸ªæ–¹æ³•æœ‰ä¸‰ä¸ªé—®é¢˜ï¼š</p>

<ul>
  <li>æ¯ä¸ª step éƒ½éœ€è¦å¯¹æ•´ä¸ªè®­ç»ƒé›†è®¡ç®—æ¢¯åº¦ï¼Œé€Ÿåº¦å¾ˆæ…¢ï¼›</li>
  <li>ç°åœ¨çš„æ•°æ®é›†éƒ½è¾ƒå¤§ï¼Œå†…å­˜è£…ä¸ä¸‹æ•´ä¸ªè®­ç»ƒé›†ï¼›</li>
  <li>ä¸èƒ½è¿›è¡Œ online trainingï¼Œä¾‹å¦‚å·²ç»éƒ¨ç½²å¥½çš„ä¸€ä¸ªæ¨¡å‹ï¼Œä¸èƒ½ç”¨æ–°æ ·æœ¬è°ƒæ•´æ¨¡å‹ã€‚</li>
</ul>

<p>Batch Gradient Descent çš„ä»£ç å½¢å¼é€šå¸¸å¦‚ä¸‹ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
   <span class="n">params_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
   <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params_grad</span>
</code></pre></div></div>

<p>é¦–å…ˆè®¡ç®—å‚æ•°å…³äºæ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦ï¼Œç„¶åå†ç”¨æ¢¯åº¦ä¹˜ä»¥å­¦ä¹ ç‡æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¦‚æœ loss å‡½æ•°æ˜¯å‡¸é¢ï¼Œæ‰¹é‡æ¢¯åº¦ä¸‹é™èƒ½ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜çš„å‚æ•°ï¼Œå¦åˆ™æ‰¾åˆ°çš„å¯èƒ½æ˜¯å±€éƒ¨æœ€ä¼˜å€¼ã€‚</p>

<h3 id="12-stochastic-gradient-descent">1.2 Stochastic Gradient Descent</h3>

<p>SGD ä¹Ÿç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ï¼Œå’Œ BGD æ˜¯ä¸¤ä¸ªæç«¯ï¼ŒBGD ä¸€ä¸ª step ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†ï¼ŒSGD ä¸€æ¬¡åªç”¨ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ $(x^{(i)}, y^{(i)})$ ï¼š</p>

\[\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})\]

<p>SGDæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š</p>

<ul>
  <li>é€Ÿåº¦å¿«ï¼Œæ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬ï¼›</li>
  <li>å¯ä»¥è¿›è¡Œ online trainingã€‚</li>
</ul>

<p>SGD ä¹Ÿæœ‰ä¸€ä¸ªæ¯”è¾ƒå¤§çš„ç¼ºç‚¹ï¼Œå°±æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œloss å‡½æ•°çš„æ³¢åŠ¨å¾ˆå¤§ï¼Œå¯¼è‡´è¾ƒé«˜çš„ varianceï¼š</p>

<p><img src="/assets/images/post_imgs/MLNotes_Optimization.assets/sgd_fluctuation.png" alt="sgd_fluctuation"></p>

<p>å¾ˆå®¹æ˜“å°±èƒ½æƒ³è±¡ï¼ŒSGD çš„è¿™ç§æ³¢åŠ¨ä½¿å¾—æ¨¡å‹å®¹æ˜“è¶Šè¿‡æœ€ä¼˜ç‚¹ã€‚ä½†æ˜¯å®éªŒè¡¨æ˜ï¼Œå¦‚æœåœ¨æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­é€æ¸å‡å°å­¦ä¹ ç‡ï¼Œæœ€ç»ˆå‡ ä¹å’Œ BGD ä¸€æ ·èƒ½å¤Ÿåˆ°è¾¾æœ€ä¼˜ç‚¹ (convex loss) æˆ–è€…å±€éƒ¨æœ€ä¼˜ç‚¹ (non-convex loss)ã€‚ä¼ªä»£ç å¦‚ä¸‹ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">params_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">example</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params_grad</span>
</code></pre></div></div>

<p>æ¯ä¸ª epoch éƒ½å¯¹æ•°æ®é›†è¿›è¡Œäº† <a href="https://stats.stackexchange.com/a/311318">shuffle</a>ï¼Œç„¶åæ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æ›´æ–°å‚æ•°ã€‚</p>

<h3 id="13-mini-batch-gradient-descent">1.3 Mini-batch Gradient Descent</h3>

<p>ä¸­æ–‡ç§°å…¶ä¸ºå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œæˆ‘ä»¬å°±ç®€ç§° MBGD å§ã€‚</p>

\[\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})\]

<p>MBGD æ¯ä¸ª step åªä½¿ç”¨è®­ç»ƒé›†çš„ä¸€å°éƒ¨åˆ†ï¼Œæ ¹æ®æ•°æ®é›†å¤§å°ã€è®¾å¤‡æ€§èƒ½ã€å†…å­˜ã€æ˜¾å­˜å®¹é‡ç­‰ï¼Œä¸€èˆ¬å– 32ï¼Œ 64ï¼Œ 128ï¼Œ 256ï¼Œ 512 ç­‰ã€‚å®ƒç»¼åˆäº† BGD å’Œ SGD çš„ä¼˜ç‚¹ï¼š</p>

<ul>
  <li>loss å’Œå‚æ•°ä¸ä¼šæœ‰å¾ˆå¤§çš„æ³¢åŠ¨ï¼›</li>
  <li>è®­ç»ƒé€Ÿåº¦å¿« (ç›®å‰çš„è®­ç»ƒæ¡†æ¶èƒ½é«˜æ•ˆåœ°è®¡ç®—æ¢¯åº¦)ï¼›</li>
  <li>å¯ä»¥ online training (åªéœ€é›†é½ä¸€ä¸ªbatchå°±å¯ä»¥)ã€‚</li>
</ul>

<p>MBGD ä¹Ÿèƒ½è¾¾æœ€ä¼˜ç‚¹ (convex loss) æˆ–è€…å±€éƒ¨æœ€ä¼˜ç‚¹ (non-convex loss)ã€‚ä¼ªä»£ç ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_epochs</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">get_batches</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span> <span class="c1"># æ¯ä¸ªstepå–50ä¸ªæ ·æœ¬
</span>        <span class="n">params_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_function</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params_grad</span>
</code></pre></div></div>

<h2 id="2-é—®é¢˜æŒ‘æˆ˜">2 é—®é¢˜æŒ‘æˆ˜</h2>

<p>Mini-batch gradient descent è™½ç„¶ä¸€å®šç¨‹åº¦ä¸Šå…‹æœäº† BGD å’Œ SGD çš„ä¸€äº›ç¼ºç‚¹ï¼Œä½†æ˜¯è¿˜é¢ä¸´å¦‚ä¸‹é—®é¢˜ï¼š</p>

<ul>
  <li>éš¾ä»¥é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ï¼Œè¿‡å°å¯¼è‡´è®­ç»ƒé€Ÿåº¦æ…¢ï¼Œè¿‡å¤§å¯¼è‡´ loss æ³¢åŠ¨ç”šè‡³å‘æ•£ï¼›</li>
  <li>å¯ä»¥å¯¹å­¦ä¹ ç‡åŠ¨æ€è°ƒæ•´ï¼Œå¦‚è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å‡å°å­¦ä¹ ç‡ï¼Œä½†æ˜¯å¦‚ä½•å‡å°ï¼Œå‡å°å¤šå°‘ä¹Ÿæ˜¯éœ€è¦æå‰å®šä¹‰çš„ã€‚è€Œæ•°æ®é›†çš„ä¸åŒå¯èƒ½éœ€è¦ä¸åŒçš„è°ƒæ•´ç­–ç•¥ï¼›</li>
  <li>åŒä¸€ä¸ªå­¦ä¹ ç‡å¯¹ä¸åŒå‚æ•°çš„æ•ˆæœå¯èƒ½å¤§ç›¸å¾„åº­ï¼Œå¦‚æœæ•°æ®é›†å„ä¸ªç‰¹å¾çš„é¢‘ç‡ (å‡ºç°æ¬¡æ•°) å·®å¼‚è¾ƒå¤§ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³å¯¹å®ƒä»¬é‡‡å–åŒæ ·çš„æ›´æ–°ç­–ç•¥ã€‚ä¾‹å¦‚é¢‘ç‡è¾ƒå°çš„ç‰¹å¾ï¼Œå¯èƒ½éœ€è¦è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼›</li>
  <li>ä¸€ä¸ªè¾ƒå¤§çš„é—®é¢˜æ˜¯ç›®å‰å¾ˆå¤šæ¨¡å‹ (ä½¿ç”¨ç¥ç»ç½‘ç»œ) éƒ½æ˜¯ non-convex çš„ï¼Œæ‰€ä»¥æ¨¡å‹å¾ˆå¯èƒ½è¢«è®­ç»ƒåˆ°ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜ç‚¹ã€‚å¦å¤–ï¼Œæ¨¡å‹å¦‚æœè½å…¥<a href="https://en.wikipedia.org/wiki/Saddle_point">éç‚¹</a> (saddle points)ï¼Œå°±å¾ˆéš¾ç»§ç»­è®­ç»ƒï¼Œå› ä¸ºå„ä¸ªæ–¹å‘çš„æ¢¯åº¦éƒ½å‡ ä¹ä¸º 0ã€‚</li>
</ul>

<h2 id="3-æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•">3 æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•</h2>

<p>æ¥ä¸‹æ¥ä»‹ç»å®è·µä¸­å¸¸ç”¨äºåº”å¯¹ä¸Šè¿°é—®é¢˜çš„æ–¹æ³• (ä¸åŒ…æ‹¬å¯¹é«˜ç»´æ•°æ®è®¡ç®—å¤æ‚åº¦è¾ƒå¤§çš„æ–¹æ³•ï¼Œå¦‚äºŒé˜¶ç‰›é¡¿æ³•)ã€‚</p>

<h3 id="31-momentum">3.1 Momentum</h3>

<p>Momentum åˆç§°ä¸ºåŠ¨é‡æ³•ï¼Œå…¶æ€è·¯å¾ˆç®€å•ã€‚æƒ³è±¡ä¸€ä¸ªå°çƒä»é«˜å¤„æ»šè½ï¼Œå°çƒçš„é€Ÿåº¦æ˜¯çŸ¢é‡ï¼Œå¯ä»¥åˆ†è§£ä¸ºå„ä¸ªæ–¹å‘çš„åˆ†é‡ï¼Œä¸€ç›´ä¸‹é™çš„æ–¹å‘çš„é€Ÿåº¦åˆ†é‡ä¼šè¶Šæ¥è¶Šå¤§ï¼Œè€Œæœ‰ä¸‹é™åˆæœ‰ä¸Šå‡çš„æ–¹å‘ï¼Œé€Ÿåº¦åˆ†é‡ä¼šç¼“æ…¢å¢åŠ ã€ç”šè‡³å‡å°‘ã€‚ä»ä¸‹é¢çš„ç¤ºæ„å›¾ä¸­å¯ä»¥çœ‹åˆ° momentum æ–¹æ³•é€Ÿåº¦éå¸¸å¿«ã€‚</p>

<p><img src="/assets/images/post_imgs/MLNotes_Optimization.assets/sgd_comparison.gif" alt="GitHub - ilguyi/optimizers.numpy"></p>

<p>åœ¨ SGD with moment æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ç»´æŒä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯å‘é‡ $v$ (ç±»æ¯”å°çƒé€Ÿåº¦çŸ¢é‡)ï¼Œå…¬å¼å¦‚ä¸‹ï¼š</p>

\[\begin{align}
\begin{split}
v_t &amp;= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\
\theta &amp;= \theta - v_t
\end{split}
\end{align}\]

<p>æœ‰äº›æ–‡ç« ä¸­çš„ç¬¦å·æ˜¯ç›¸åçš„ï¼Œä¹Ÿæ˜¯æ­£ç¡®çš„ï¼š</p>

\[\begin{align}
\begin{split}
v_t &amp;= \gamma v_{t-1} - \eta \nabla_\theta J( \theta) \\
\theta &amp;= \theta + v_t
\end{split}
\end{align}\]

<p>$\gamma$ æ˜¯ momentum çš„æƒé‡ï¼Œä¸€èˆ¬å– 0.9ã€‚</p>

<h3 id="32-nesterov-accelerated-gradient">3.2 â€‹Nesterov Accelerated Gradient</h3>

<p>Nesterov Accelerated Gradient(NAG) å¯¹ momentum åšäº†ä¸€ç‚¹æ”¹è¿›ï¼š</p>

\[\begin{align}
\begin{split}
v_t &amp;= \gamma v_{t-1} + \eta \nabla_\theta J(\theta-\gamma v_{t-1}) \\
\theta &amp;= \theta - v_t
\end{split}
\end{align}\]

<p>NAG å…ˆå¾€å‰è¿ˆä¸€æ­¥ï¼Œå³ $\theta-\gamma v_{t-1}$ï¼Œç„¶åè®¡ç®—æ¢¯åº¦ï¼Œæ¥ç€ç”¨è®¡ç®—å‡ºçš„æ¢¯åº¦ä¿®æ­£å½“å‰çš„æ¢¯åº¦ç´¯ç§¯é‡ã€‚è€Œ momentum æ˜¯å…ˆåœ¨å½“å‰ä½ç½®è®¡ç®—æ¢¯åº¦ï¼Œç„¶åå†è·³ä¸€å¤§æ­¥æ›´æ–°ä½ç½®ã€‚</p>

<p><img src="/assets/images/post_imgs/MLNotes_Optimization.assets/nesterov.png" alt="nesterov"></p>

<p>ä¸Šå›¾ä¸­ï¼Œç»¿è‰²ä¸ºæœ€ä½³è·¯çº¿ï¼Œè“è‰²ä¸º momentum è·¯çº¿ (å…ˆè®¡ç®—æ¢¯åº¦ï¼Œç´¯è®¡ï¼Œå†è·³ä¸€å¤§æ­¥)ï¼Œè¤è‰² + çº¢è‰²ä¸º NAG è·¯çº¿ (å…ˆè·³ä¸€å¤§æ­¥ï¼Œå†è®¡ç®—æ¢¯åº¦ï¼Œä¿®æ­£)ã€‚</p>

<h3 id="33-adagrad">3.3 AdaGrad</h3>

<p>â€AdaGradâ€ â€å¯ä½¿å­¦ä¹ é€Ÿç‡é€‚åº”å‚æ•°ã€‚å®ƒå¯¹æ ·æœ¬ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„ç‰¹å¾å¯¹åº”çš„å‚æ•°æ‰§è¡Œè¾ƒå°çš„æ›´æ–°ï¼Œå¯¹é¢‘ç‡è¾ƒä½çš„ç‰¹å¾å¯¹åº”çš„å‚æ•°æ‰§è¡Œè¾ƒå¤§çš„æ›´æ–°ã€‚å…¶æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š</p>

<ol>
  <li>
    <p>åœ¨æ­¥éª¤ tï¼Œç”¨ $s_t$ ä¿å­˜å„ä¸ªå‚æ•°æ¯ä¸ª step çš„æ¢¯åº¦çš„å¹³æ–¹å’Œï¼š</p>

\[s_t = s_{t-1} + g_t\odot g_t\]

    <p>è¿™é‡Œ $\odot$ æ˜¯å…ƒç´ å¯¹åº”ç›¸ä¹˜ï¼Œå³è®¡ç®—æ¢¯åº¦å‘é‡æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹ã€‚å¯ä»¥çœ‹å‡ºï¼Œå¦‚æœæŸä¸ªå‚æ•°ä¸‹é™å¾ˆå¿«ï¼Œé‚£ä¹ˆ $s_t$ å¯¹åº”åˆ†é‡çš„ç´¯è®¡å°±è¶Šå¤§ã€‚</p>
  </li>
  <li>
    <p>æ›´æ–°å‚æ•°ï¼š</p>

\[\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t\]

    <p>å› ä¸ºä¸‹é™è¶Šå¿«çš„å‚æ•° $s_t$ ä¸­çš„åˆ†é‡è¶Šå¤§ï¼Œé‚£ä¹ˆ $\frac{\eta}{\sqrt{s_t + \epsilon}}$ å°±ä¼šè¶Šå°ï¼Œå³å­¦ä¹ é€Ÿç‡å°±æ›´å°ï¼Œåä¹‹ä¸‹é™æ…¢çš„å‚æ•°ï¼Œå¯¹åº”çš„å­¦ä¹ é€Ÿç‡å°±é«˜ä¸€ç‚¹ã€‚</p>
  </li>
</ol>

<p>è¿™é‡Œçš„ $\epsilon$ æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ­£å€¼ï¼Œé˜²æ­¢è®¡ç®—è¿‡ç¨‹ä¸­é™¤é›¶ (é€šå¸¸å– $1e-8$ çº§åˆ«çš„æ•°)ã€‚å¦å¤–ä¸€ç‚¹æ˜¯ä¸å¼€æ–¹çš„è¯ï¼Œæ•ˆæœä¼šå¾ˆå·®ã€‚</p>

<p>AdaGrad çš„å¥½å¤„æ˜¯ä¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡ã€‚ä½†æœ‰ä¸ªç¼ºç‚¹ï¼šç”±äºç´¯ç§¯æ¢¯åº¦å‘é‡ $s_t$ çš„å…ƒç´ æ¯ä¸ªæ­¥éª¤æ–°å¢é¡¹éƒ½æ˜¯æ­£æ•° (è¯¥åˆ†é‡æ¢¯åº¦çš„å¹³æ–¹)ï¼Œå› æ­¤åœ¨è®­ç»ƒæœŸé—´ç´¯ç§¯çš„æ€»å’Œä¸æ–­å¢åŠ ï¼Œ $\frac{\eta}{\sqrt{s_t + \epsilon}}$ ä¸æ–­å‡å°ï¼Œå¯¼è‡´å­¦ä¹ ç‡ç¼©å°å¹¶å˜å¾—æå° (å­¦ä¹ ç‡æ¶ˆå¤±)â€ã€‚</p>

<p>AdaGrad äº 2011 å¹´è¢«æå‡ºï¼Œç”±äºå­¦ä¹ ç‡æ¶ˆå¤±çš„é—®é¢˜ï¼Œå·²ç»åŸºæœ¬æ— äººé‡‡ç”¨ï¼Œå…·ä½“ç»†èŠ‚å¯ä»¥é˜…è¯»è®ºæ–‡ â€Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121â€“2159.â€œã€‚</p>

<h3 id="34-adadelta">3.4 AdaDelta</h3>

<p>AdaDelta æ˜¯ â€AdaGradâ€ çš„æ”¹è‰¯ç‰ˆæœ¬ï¼Œç›®çš„æ˜¯è§£å†³æ¢¯åº¦å¹³æ–¹ç´¯è®¡å¯¼è‡´çš„å­¦ä¹ é€Ÿç‡æå°(æ¶ˆå¤±)çš„é—®é¢˜ã€‚å…¶æ€è·¯å¾ˆç®€å•ï¼Œç»´æŠ¤ä¸€ä¸ªç´¯ç§¯çª—å£ï¼Œè€Œä¸æ˜¯ç´¯ç§¯æ¯ä¸€æ­¥çš„æ¢¯åº¦ã€‚å…·ä½“å‚è€ƒï¼š<a href="https://paperswithcode.com/method/adadelta">AdaDelta Explained</a>ã€‚</p>

<p>å››ç§æ–¹æ³•æ¯”è¾ƒï¼š</p>

<p><img src="/assets/images/post_imgs/MLNotes_Optimization.assets/sgd_adagrad.png" alt="sgd_adagrad"></p>

<h3 id="35-rmsprop">3.5 RMSProp</h3>

<p>RMSprop æœªç»è®ºæ–‡å‘è¡¨ï¼Œæ˜¯ä¸€ç§é€‚åº”æ€§çš„å­¦ä¹ é€Ÿç‡è°ƒæ•´æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”± Geoff Hinton åœ¨ä»–çš„ Coursera <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">è¯¾ç¨‹è®²ä¹‰</a>ä¸­æå‡ºï¼Œå’Œ AdaDelta çš„ç›®çš„ç›¸åŒï¼Œä¹Ÿæ˜¯ä¸ºäº†è§£å†³ AdaGrad å­¦ä¹ ç‡æ¶ˆå¤±é—®é¢˜ã€‚</p>

<p>RMSProp çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š</p>

<ol>
  <li>
    <p>åœ¨æ­¥éª¤ tï¼Œè®¡ç®—ï¼š</p>

\[s_t = \gamma s_{t-1} + (1-\gamma)g_t\odot g_t\]
  </li>
  <li>
    <p>æ›´æ–°å‚æ•°ï¼š</p>

\[\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t\]
  </li>
</ol>

<p>æˆ‘ä»¬æŠŠç¬¬ä¸€æ­¥è®¡ç®— $s_t$ çš„å…¬å¼å±•å¼€ï¼š</p>

\[\begin{split}\begin{aligned}
\mathbf{s}_t &amp; = (1 - \gamma) \mathbf{g}_t^2 + \gamma \mathbf{s}_{t-1} \\
&amp; = (1 - \gamma) \left(\mathbf{g}_t^2 + \gamma \mathbf{g}_{t-1}^2 + \gamma^2 \mathbf{g}_{t-2} + \ldots, \right).
\end{aligned}\end{split}\]

<p>æˆ‘ä»¬çŸ¥é“ $1 + \gamma + \gamma^2 + \ldots = \frac{1}{1-\gamma}$ï¼Œå³ $s_t$ æ˜¯ä¹‹å‰æ­¥éª¤ç´¯ç§¯æ¢¯åº¦çš„æœŸæœ›å€¼ï¼Œå› ä¸º $\gamma &lt; 1$ï¼Œè¶Šæ—©çš„æ¢¯åº¦ç´¯è®¡å¯¹å½“å‰çš„å½±å“è¶Šå°ï¼Œå¾ˆå¤šæ­¥ä¹‹å‰çš„æ¢¯åº¦ç´¯è®¡ç”šè‡³å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œæ•…ä¸ä¼šå‡ºç° $s_t$ æ— é™åˆ¶å¢é•¿ï¼Œå¯¼è‡´å­¦ä¹ ç‡æ¶ˆå¤±çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ $\gamma$ æ¥æ§åˆ¶ $s_t$ å¯¹å†å²æ¢¯åº¦çš„ç´¯ç§¯èŒƒå›´ã€‚ä¸‹é¢çš„å›¾ç‰‡æ˜¯ä¸åŒ $\gamma$ å–å€¼å¯¹åº”çš„å†å²æ¢¯åº¦ç´¯è®¡çš„æƒé‡ï¼š</p>

<p><img src="/assets/images/post_imgs/MLNotes_Optimization.assets/rmsprop_gamma.png" alt="rmsprop_gamma"></p>

<p>ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œå–å€¼è¶Šå¤§ï¼Œ$s_t$ è¶Šâ€å…³æ³¨â€œå†å²ç§¯ç´¯çš„æ¢¯åº¦ã€‚40 æ­¥ä¹‹å‰çš„ç§¯ç´¯ï¼Œè™½ç„¶åœ¨è§£æå¼ä¸­ä¸ä¸º 0ï¼Œä½†åœ¨å†…å­˜ä¸­æ•°å€¼ä¸Šå·²ç»ä¸º 0ã€‚</p>

<h3 id="36-adam">3.6 Adam</h3>

<p>Adam æ˜¯ RMSProp å’Œ Momentum çš„ç»“åˆã€‚</p>

\[\begin{split}\begin{aligned}
    \mathbf{v}_t &amp; \leftarrow \beta_1 \mathbf{v}_{t-1} + (1 - \beta_1) \mathbf{g}_t \\
    \mathbf{s}_t &amp; \leftarrow \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t \odot \mathbf{g}_t
\end{aligned}\end{split}\]

<p>ç¬¬ä¸€è¡Œå…¬å¼å³ Momentumï¼Œé€šå¸¸ $\beta_1 = 0.9$ã€‚ç¬¬äºŒè¡Œå³ RMSPropï¼Œé€šå¸¸ $\beta_2 = 0.999$ã€‚</p>

<p>åœ¨åˆšå¼€å§‹æ—¶æˆ‘ä»¬æŠŠ $\mathbf{v}_t$ å’Œ $\mathbf{s}_t$ è®¾ä¸º 0ï¼Œå³ $\mathbf{v}_0 = \mathbf{s}_0 = 0$ï¼Œé‚£ä¹ˆæœ‰ $\mathbf{v}_1 = 0.1 \mathbf{g}_1$ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„æ¢¯åº¦å€¼ã€‚$\mathbf{s}_1$ ä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ã€‚æ•…è¶Šæ—©çš„æ—¶é—´æ­¥ï¼Œåå·®è¶Šå¤§ï¼Œå› æ­¤æˆ‘ä»¬è¦ä¿®æ­£åå·®ã€‚</p>

<p>åœ¨æ—¶é—´æ­¥ t æˆ‘ä»¬å¾—åˆ°</p>

\[\boldsymbol{v}_t = (1-\beta_1) \sum_{i=1}^{t} \beta_1^{t-i} \boldsymbol{g}_i\]

<p>(ä¸Šé¢ç¬¬ä¸€ä¸ªå¼å­å±•å¼€)ï¼Œå°†è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦çš„æƒå€¼ç›¸åŠ ï¼Œå¾—åˆ°</p>

\[(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t\]

<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“ t è¾ƒå°æ—¶ï¼Œè¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¼šè¾ƒå°ã€‚ä¾‹å¦‚ï¼Œå½“ $\beta_1 = 0.9$ æ—¶ï¼Œ$\mathbf{v}_1 = 0.1 \mathbf{g}_1$ï¼Œæ‰€æœ‰æƒå€¼ä¹‹å’Œä¸º 0.1ã€‚ä¸ºäº†æ¶ˆé™¤è¿™æ ·çš„å½±å“ï¼Œå¯¹äºä»»æ„æ—¶é—´æ­¥ tï¼Œæˆ‘ä»¬å¯ä»¥å°† $\mathbf{v}_t$ å†é™¤ä»¥$1 - \beta_1^t$ï¼Œä»è€Œä½¿è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¸º1ã€‚è¿™ä¹Ÿå«ä½œåå·®ä¿®æ­£ã€‚åœ¨Adamç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å¯¹å˜é‡å‡ $\mathbf{v}_t$ å’Œ $\mathbf{s}_t$ ä½œåå·®ä¿®æ­£ï¼š</p>

\[\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t},\]

\[\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}.\]

<p>æœ€åè¿›è¡Œæ›´æ–°ï¼š</p>

\[\boldsymbol{x}_t \leftarrow
\boldsymbol{x}_t - \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon},\]

<p>å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\epsilon$ æ˜¯ä¸ºäº†ç»´æŒæ•°å€¼ç¨³å®šæ€§è€Œæ·»åŠ çš„å¸¸æ•°ï¼Œå¦‚ $10^{âˆ’8}$ã€‚å’Œ AdaGrad ç®—æ³•ã€RMSProp ç®—æ³•ä»¥åŠAdaDelta ç®—æ³•ä¸€æ ·ï¼Œç›®æ ‡å‡½æ•°è‡ªå˜é‡ä¸­æ¯ä¸ªå…ƒç´ éƒ½åˆ†åˆ«æ‹¥æœ‰è‡ªå·±çš„å­¦ä¹ ç‡ã€‚</p>

<hr>

<p>ğŸ‘å¼ºçƒˆæ¨èä»¥ä¸‹è§†é¢‘ï¼Œè®²äº†Momentum, Adagrad, RMSProp, Adam çš„åŸç†å’Œæ¼”è¿›å…³ç³»ï¼š</p>

<ul>
  <li>Bç«™(æ— å­—å¹•)ï¼š<a href="https://www.bilibili.com/video/BV134411q793?share_source=copy_web">https://www.bilibili.com/video/BV134411q793?share_source=copy_web</a>
</li>
  <li>æ²¹ç®¡(æœ‰è‡ªåŠ¨ç”Ÿæˆå­—å¹•)ï¼š<a href="https://www.youtube.com/watch?v=gmwxUy7NYpA">L26/1 Momentum, Adagrad, RMSProp, Adam - YouTube</a>
</li>
</ul>

<hr>

<h3 id="37-å…¶ä»–ä¼˜åŒ–å™¨">3.7 å…¶ä»–ä¼˜åŒ–å™¨</h3>

<p>è¯¥ç¯‡ <a href="https://ruder.io/optimizing-gradient-descent/">review</a> è¿˜è®²è§£äº†å…¶ä»–ä¼˜åŒ–å™¨ï¼Œç”±äºåœ¨å®è·µä¸­ï¼Œé™¤éæ˜¯ä¸“é—¨ç ”ç©¶ä¼˜åŒ–å™¨ï¼ŒåŸºæœ¬ä¸ä¼šä½¿ç”¨å®ƒä»¬ï¼Œæ‰€ä»¥æœ¬æ–‡èŠ‚çº¦æ—¶é—´ç‚¹åˆ°ä¸ºæ­¢ã€‚è¿™äº›ä¼˜åŒ–å™¨å‡åœ¨ <a href="https://paperswithcode.com/methods/category/stochastic-optimization">An Overview of Stochastic Optimization</a> åˆ—å‡ºï¼Œæ„Ÿå…´è¶£å¯ä»¥æŸ¥çœ‹ã€‚ä¸‹é¢æ˜¯å„ç§ä¼˜åŒ–å™¨åœ¨éç‚¹ä¸Šçš„è¡¨ç°ï¼š</p>

<p><img src="/assets/images/post_imgs/MLNotes_Optimization.assets/saddle_point_evaluation_optimizers.gif" alt="saddle_point_evaluation_optimizers"></p>

<h3 id="38-é€‰ç”¨å“ªä¸ªä¼˜åŒ–å™¨">3.8 é€‰ç”¨å“ªä¸ªä¼˜åŒ–å™¨ï¼Ÿ</h3>

<p>è¯¥ç¯‡ <a href="https://ruder.io/optimizing-gradient-descent/">review</a> ä¸­å†™é“ï¼š</p>

<blockquote>
  <p>So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you wonâ€™t need to tune the learning rate but likely achieve the best results with the default value.</p>

  <p>In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [<a href="https://ruder.io/optimizing-gradient-descent/index.html#fn14">14:1]</a> show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, ==Adam might be the best overall choice==.</p>

  <p>Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.</p>

  <p>â€‹                                                                                                  â€“  â€œAn overview of gradient descent optimization algorithmsâ€</p>
</blockquote>

<p>å¤§å¤šæ•°æƒ…å†µç”¨ Adam å°±å¯¹äº†ï¼</p>

<h2 id="4-å…¶ä»–ä¼˜åŒ–ç­–ç•¥">4 å…¶ä»–ä¼˜åŒ–ç­–ç•¥</h2>

<p>è¯¥ç¯‡ <a href="https://ruder.io/optimizing-gradient-descent/index.html#additionalstrategiesforoptimizingsgd">review</a> ä¸­æåˆ°çš„å…³äº SGD çš„å…¶ä»–ä¼˜åŒ–ç­–ç•¥åŒ…æ‹¬ï¼š</p>

<ul>
  <li><a href="https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network">Shuffling</a></li>
  <li><a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/">Batch Normalization</a></li>
  <li><a href="https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/">Early Stopping</a></li>
  <li><a href="https://arxiv.org/abs/1511.06807">Gradient Noise</a></li>
</ul>

<p>Shuffling å’Œ Batch Normalization æ˜¯å®è·µä¸­å¸¸ç”¨çš„ã€‚</p>

<h2 id="æ¨èé˜…è¯»">æ¨èé˜…è¯»</h2>

<ol>
  <li><a href="https://ruder.io/optimizing-gradient-descent/index.html">An overview of gradient descent optimization algorithms (ruder.io)</a></li>
  <li><a href="https://paperswithcode.com/methods/category/stochastic-optimization">An Overview of Stochastic Optimization</a></li>
  <li><a href="https://d2l.ai/chapter_optimization/index.html">11. Optimization Algorithms â€” Dive into Deep Learning 0.16.6 documentation (d2l.ai)</a></li>
  <li><a href="https://distill.pub/2017/momentum/">Why Momentum Really Works (distill.pub)</a></li>
  <li><a href="https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network">Why should we shuffle data while training a neural network?</a></li>
  <li><a href="https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/">A Gentle Introduction to Batch Normalization for Deep Neural Networks (machinelearningmastery.com)</a></li>
  <li><a href="https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/">A Gentle Introduction to Early Stopping to Avoid Overtraining Neural Networks (machinelearningmastery.com)</a></li>
</ol>

<h2 id="å‚è€ƒæ–‡çŒ®">å‚è€ƒæ–‡çŒ®</h2>

<ol>
  <li>H. Robinds and S. Monro, â€œA stochastic approximation method,â€ Annals of Mathematical Statistics, vol. 22, pp. 400â€“407, 1951. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref1">â†©ï¸</a>
</li>
  <li>Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1â€“11. <a href="https://hub.pubmedplus.com/10.1109/NNSP.1992.253713">http://doi.org/10.1109/NNSP.1992.253713</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref2">â†©ï¸</a>
</li>
  <li>Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1â€“14. Retrieved from <a href="https://arxiv.org/abs/1406.2572">http://arxiv.org/abs/1406.2572</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref3">â†©ï¸</a>
</li>
  <li>Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref4">â†©ï¸</a>
</li>
  <li>Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145â€“151. <a href="https://hub.pubmedplus.com/10.1016/S0893-6080(98)00116-6">http://doi.org/10.1016/S0893-6080(98)00116-6</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref5">â†©ï¸</a>
</li>
  <li>Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543â€“ 547. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref6">â†©ï¸</a>
</li>
  <li>Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from <a href="https://arxiv.org/abs/1212.0901">http://arxiv.org/abs/1212.0901</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref7">â†©ï¸</a>
</li>
  <li>Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref8">â†©ï¸</a>
</li>
  <li>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121â€“2159. Retrieved from <a href="http://jmlr.org/papers/v12/duchi11a.html">http://jmlr.org/papers/v12/duchi11a.html</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref9">â†©ï¸</a>
</li>
  <li>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, â€¦ Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1â€“11. <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref10">â†©ï¸</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref10:1">â†©ï¸</a>
</li>
  <li>Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532â€“1543. <a href="https://hub.pubmedplus.com/10.3115/v1/D14-1162">http://doi.org/10.3115/v1/D14-1162</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref11">â†©ï¸</a>
</li>
  <li>Duchi et al. [3] give this matrix as an alternative to the <em>full</em> matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters dd. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref12">â†©ï¸</a>
</li>
  <li>Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from <a href="https://arxiv.org/abs/1212.5701">http://arxiv.org/abs/1212.5701</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref13">â†©ï¸</a>
</li>
  <li>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1â€“13. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref14">â†©ï¸</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref14:1">â†©ï¸</a>
</li>
  <li>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., &amp; Hochreiter, S. (2017). GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems 30 (NIPS 2017). <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref15">â†©ï¸</a>
</li>
  <li>Dozat, T. (2016). Incorporating Nesterov Momentum into Adam. ICLR Workshop, (1), 2013â€“2016. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref16">â†©ï¸</a>
</li>
  <li>Huang, G., Liu, Z., Weinberger, K. Q., &amp; van der Maaten, L. (2017). Densely Connected Convolutional Networks. In Proceedings of CVPR 2017. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref17">â†©ï¸</a>
</li>
  <li>Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., â€¦ Dean, J. (2016). Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref18">â†©ï¸</a>
</li>
  <li>Reddi, Sashank J., Kale, Satyen, &amp; Kumar, Sanjiv. On the Convergence of Adam and Beyond. Proceedings of ICLR 2018. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref19">â†©ï¸</a>
</li>
  <li>Loshchilov, I., &amp; Hutter, F. (2019). Decoupled Weight Decay Regularization. In Proceedings of ICLR 2019. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref20">â†©ï¸</a>
</li>
  <li>Ma, J., &amp; Yarats, D. (2019). Quasi-hyperbolic momentum and Adam for deep learning. In Proceedings of ICLR 2019. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref21">â†©ï¸</a>
</li>
  <li>Lucas, J., Sun, S., Zemel, R., &amp; Grosse, R. (2019). Aggregated Momentum: Stability Through Passive Damping. In Proceedings of ICLR 2019. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref22">â†©ï¸</a>
</li>
  <li>Niu, F., Recht, B., Christopher, R., &amp; Wright, S. J. (2011). Hogwild! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1â€“22. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref23">â†©ï¸</a>
</li>
  <li>Mcmahan, H. B., &amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1â€“9. Retrieved from <a href="http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf">http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref24">â†©ï¸</a>
</li>
  <li>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., â€¦ Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref25">â†©ï¸</a>
</li>
  <li>Zhang, S., Choromanska, A., &amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1â€“24. Retrieved from <a href="https://arxiv.org/abs/1412.6651">http://arxiv.org/abs/1412.6651</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref26">â†©ï¸</a>
</li>
  <li>LeCun, Y., Bottou, L., Orr, G. B., &amp; MÃ¼ller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9â€“50. <a href="https://hub.pubmedplus.com/10.1007/3-540-49430-8_2">http://doi.org/10.1007/3-540-49430-8_2</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref27">â†©ï¸</a>
</li>
  <li>Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41â€“48. <a href="https://hub.pubmedplus.com/10.1145/1553374.1553380">http://doi.org/10.1145/1553374.1553380</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref28">â†©ï¸</a>
</li>
  <li>Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1â€“25. Retrieved from <a href="https://arxiv.org/abs/1410.4615">http://arxiv.org/abs/1410.4615</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref29">â†©ï¸</a>
</li>
  <li>Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3. <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref30">â†©ï¸</a>
</li>
  <li>Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1â€“11. Retrieved from <a href="https://arxiv.org/abs/1511.06807">http://arxiv.org/abs/1511.06807</a> <a href="https://ruder.io/optimizing-gradient-descent/index.html#fnref31">â†©ï¸</a>
</li>
</ol>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/2021/activations" title="æœºå™¨å­¦ä¹ ç¬”è®° - æ¿€æ´»å‡½æ•°æ€»ç»“">æœºå™¨å­¦ä¹ ç¬”è®° - æ¿€æ´»å‡½æ•°æ€»ç»“</a><a class="next" href="/2021/cs231n_summary" title="CS231nå­¦ä¹ æ€»ç»“">CS231nå­¦ä¹ æ€»ç»“</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li><a class="post-link" href="/2021/activations" title="CS231nå­¦ä¹ æ€»ç»“">æœºå™¨å­¦ä¹ ç¬”è®° - æ¿€æ´»å‡½æ•°æ€»ç»“</a></li>
<li><a class="post-link" href="/2021/optimization" title="CS231nå­¦ä¹ æ€»ç»“">æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“</a></li>
<li><a class="post-link" href="/2021/cs231n_summary" title="CS231nå­¦ä¹ æ€»ç»“">CS231nå­¦ä¹ æ€»ç»“</a></li>
<li><a class="post-link" href="/2021/softmax" title="CS231nå­¦ä¹ æ€»ç»“">æœºå™¨å­¦ä¹ ç¬”è®° - Softmax</a></li>
</ul>
    </div>
<div class="post-comments">  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://xietx1995.github.io/2021/optimization';
      this.page.identifier = 'https://xietx1995.github.io/2021/optimization';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://xietx1995.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
</div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
      <div>Copyright Â© 2021 @Tianxin Xie</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="http://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
    </div>
  </div>
</footer>
<script src="/assets/js/vanilla-back-to-top.min.js"></script>
  <script>addBackToTop({
      diameter: 56,
      backgroundColor: '#ff5100',
      textColor: '#fff'
    })</script>

</body>

</html>
