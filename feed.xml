<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://xietx1995.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://xietx1995.github.io/" rel="alternate" type="text/html" /><updated>2021-07-16T17:06:46+08:00</updated><id>https://xietx1995.github.io/feed.xml</id><title type="html">Tianxinâ€™s Blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>Tianxin Xie</name></author><entry><title type="html">CS231nå­¦ä¹ æ€»ç»“</title><link href="https://xietx1995.github.io/2021/cs231n_summary" rel="alternate" type="text/html" title="CS231nå­¦ä¹ æ€»ç»“" /><published>2021-07-15T00:00:00+08:00</published><updated>2021-07-15T00:00:00+08:00</updated><id>https://xietx1995.github.io/2021/CS231n_Summary</id><content type="html" xml:base="https://xietx1995.github.io/2021/cs231n_summary">&lt;p&gt;æ–­æ–­ç»­ç»­èŠ±äº†å¤§çº¦äº”å‘¨çš„æ—¶é—´å­¦å®Œäº† CS231nï¼Œæ˜¯æ—¶å€™æ€»ç»“ä¸€ä¸‹è‡ªå·±é‡åˆ°çš„é—®é¢˜å’Œæ”¶è·äº†ã€‚&lt;/p&gt;

&lt;h2 id=&quot;è¯¾ç¨‹ä¿¡æ¯&quot;&gt;è¯¾ç¨‹ä¿¡æ¯&lt;/h2&gt;

&lt;p&gt;è¯¾ç¨‹ä¸»é¡µï¼š&lt;a href=&quot;http://cs231n.stanford.edu/index.html&quot;&gt;Stanford University CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CS231n è§†é¢‘(2017å¹´)ï¼š&lt;a href=&quot;https://www.bilibili.com/video/BV1nJ411z7fe?share_source=copy_web&quot;&gt;https://www.bilibili.com/video/BV1nJ411z7fe?share_source=copy_web&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;æ¨èç»“åˆ &lt;a href=&quot;https://web.eecs.umich.edu/~justincj/&quot;&gt;Justin Johnson (umich.edu)&lt;/a&gt; çš„ &lt;a href=&quot;https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/schedule.html&quot;&gt;EECS 498-007 / 598-005: Deep Learning for Computer Vision (umich.edu)&lt;/a&gt; å­¦ä¹ ã€‚ï¼ˆCS231n çš„è®²å¸ˆ Justin Johnson ä»æ–¯å¦ç¦æ¯•ä¸šä¹‹åå»äº†å¯†æ­‡æ ¹å·ç«‹å¤§å­¦ä»»æ•™ï¼Œæ•™å­¦å†…å®¹ã€PPTã€å®éªŒä½œä¸šç­‰éƒ½å’Œ CS231n å·®ä¸å¤šï¼Œå…¬å¼€&lt;a href=&quot;https://www.bilibili.com/video/BV1Yp4y1q7ms?from=search&amp;amp;seid=10520157556341037278&quot;&gt;è§†é¢‘&lt;/a&gt;æ˜¯2019å¹´çš„ï¼‰&lt;/p&gt;

&lt;p&gt;æˆ‘æ˜¯æŒ‰ç…§ 2021 å¹´ &lt;a href=&quot;http://cs231n.stanford.edu/index.html&quot;&gt;CS231n&lt;/a&gt; çš„ &lt;a href=&quot;http://cs231n.stanford.edu/schedule.html&quot;&gt;Schedule&lt;/a&gt; æ¥å­¦ä¹ çš„ã€‚è™½ç„¶ CS231n çš„è¯¾ç¨‹å†…å®¹ä»¥ lecture åˆ’åˆ†ï¼Œä½†æ˜¯ç»è¿‡å­¦ä¹ ä¹‹åï¼Œæˆ‘è®¤ä¸ºå¯ä»¥åˆ†ä¸ºå››å¤§éƒ¨åˆ†ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lecture 1~4ï¼šä¸»è¦ä»‹ç»åˆ†ç±»é—®é¢˜ã€çº¿æ€§åˆ†ç±»å™¨ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–æ–¹æ³•ã€(å…¨è¿æ¥)ç¥ç»ç½‘ç»œã€åå‘ä¼ æ’­ç®—æ³•ï¼Œè¿™äº›å±äºæ·±åº¦å­¦ä¹ åŸºç¡€ä¸­çš„åŸºç¡€ï¼Œä¹Ÿæ˜¯&lt;strong&gt;ä½œä¸š1&lt;/strong&gt;çš„ä¸»è¦å†…å®¹ï¼›&lt;/li&gt;
  &lt;li&gt;Lecture 5~9ï¼šä¸»è¦ä»‹ç»å·ç§¯ç¥ç»ç½‘ç»œ(CNN)ã€æ·±åº¦å­¦ä¹ ä¸­çš„ç¡¬ä»¶å’Œè½¯ä»¶ã€ç½‘ç»œçš„è®­ç»ƒå’Œä¼˜åŒ–æ–¹æ³•ã€æ¯”è¾ƒè‘—åçš„ç½‘ç»œæ¶æ„ï¼Œè¿™éƒ¨åˆ†å±äºè®¡ç®—æœºè§†è§‰çš„å¿…å¤‡çŸ¥è¯†ï¼Œæ˜¯&lt;strong&gt;ä½œä¸š2&lt;/strong&gt;çš„ä¸»è¦å†…å®¹ï¼›&lt;/li&gt;
  &lt;li&gt;Lecture 10~15ï¼šä¸»è¦ä»‹ç»å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ã€LSTMã€Attentionæœºåˆ¶ã€Transformerç­‰ï¼Œè¿˜æœ‰ç”Ÿæˆæ¨¡å‹ã€è‡ªç›‘ç£å­¦ä¹ ã€ç½‘ç»œå¯è§†åŒ–å’Œç†è§£ã€æ£€æµ‹å’Œåˆ†å‰²ï¼Œæ˜¯&lt;strong&gt;ä½œä¸š3&lt;/strong&gt;çš„ä¸»è¦å†…å®¹ï¼›&lt;/li&gt;
  &lt;li&gt;Lecture 16~19ï¼šè¿™éƒ¨åˆ†æ˜¯å˜‰å®¾Lectureï¼Œæ²¡æœ‰è§†é¢‘ï¼Œåªæœ‰slidesã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;å¦å¤– &lt;a href=&quot;https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/schedule.html&quot;&gt;EECS 498-007 / 598-005: Deep Learning for Computer Vision (umich.edu)&lt;/a&gt; è¿˜åŒ…æ‹¬äº†3Dè§†è§‰ã€è§†é¢‘åˆ†ç±»ã€å¼ºåŒ–å­¦ä¹ ï¼Œç”Ÿæˆæ¨¡å‹éƒ¨åˆ†ä¹Ÿæ¯”CS231næ›´åŠ è¯¦ç»†ã€‚ä¸‹é¢å¯¹å„ä¸ªéƒ¨åˆ†çš„å¤§è‡´å†…å®¹å’Œé˜…è¯»è§‚çœ‹è¿‡çš„å‚è€ƒèµ„æ–™å’Œè§†é¢‘åšä¸€ä¸ªæ¢³ç†ï¼Œæ–¹ä¾¿ç”¨åˆ°çš„æ—¶å€™å†å›é¡¾ã€‚&lt;/p&gt;

&lt;h2 id=&quot;lecture-14&quot;&gt;Lecture 1~4&lt;/h2&gt;

&lt;p&gt;Lecture 1å’Œ2ä»å›¾åƒåˆ†ç±»ç€æ‰‹ï¼Œä»‹ç»äº†å›¾åƒåˆ†ç±»é—®é¢˜çš„éš¾ç‚¹å’Œæµç¨‹(pipeline)ï¼Œå¹¶ä½¿ç”¨ä»‹ç»äº†&lt;strong&gt;æœ€è¿‘é¢†(Nearest Neighbor)&lt;/strong&gt;ç®—æ³•ï¼Œç„¶åè¦æ±‚åœ¨ä½œä¸šä¸­å®ç°æ•ˆæœæ›´å¥½çš„&lt;strong&gt;KNN(K Nearest Neighbor)&lt;/strong&gt;ç®—æ³•ã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;é˜…è¯»ææ–™ï¼š&lt;a href=&quot;https://cs231n.github.io/classification/&quot;&gt;https://cs231n.github.io/classification/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lecture 3ä»¥&lt;strong&gt;CIFAR10&lt;/strong&gt;æ•°æ®é›†ä¸ºä¾‹ï¼Œä»‹ç»äº†çº¿æ€§å¤šåˆ†ç±»å™¨ã€Losså‡½æ•°ã€å¤šåˆ†ç±»&lt;strong&gt;SVM&lt;/strong&gt;ã€&lt;strong&gt;Softmax&lt;/strong&gt;ï¼Œæœ€åå¯¹SVMå’ŒSoftmaxè¿›è¡Œäº†å¯¹æ¯”ã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;é˜…è¯»ææ–™ï¼š&lt;a href=&quot;https://cs231n.github.io/linear-classify/&quot;&gt;https://cs231n.github.io/linear-classify/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;æ¥ç€å¯¹æ±‚è§£æœ€ä¼˜å‚æ•°çš„æ–¹æ³•è¿›è¡Œä»‹ç»ï¼ŒåŒ…æ‹¬æ¢¯åº¦ä¸‹é™ã€gradient checkæŠ€å·§ã€é˜²æ­¢æ•°å€¼ä¸ç¨³å®š(é™¤é›¶æˆ–è€…æ•°å€¼è¿‡å¤§)çš„æŠ€å·§ç­‰ã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;é˜…è¯»ææ–™ï¼š&lt;a href=&quot;https://cs231n.github.io/optimization-1/&quot;&gt;https://cs231n.github.io/optimization-1/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lecture 4è®²è§£äº†ç¥ç»ç½‘ç»œã€åå‘ä¼ æ’­ï¼Œæ‰‹æ¨åå‘ä¼ æ’­å¾ˆé‡è¦çš„ä¸€ä¸ªå·¥å…·å°±æ˜¯è®¡ç®—å›¾ã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;é˜…è¯»ææ–™ï¼š&lt;a href=&quot;https://cs231n.github.io/optimization-2/&quot;&gt;https://cs231n.github.io/optimization-2/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;lecture-59&quot;&gt;Lecture 5~9&lt;/h2&gt;

&lt;p&gt;è¿™éƒ¨åˆ†ä¸»è¦å›´ç»•CNNå±•å¼€ï¼ŒLecture 5è®²è§£äº†CNNåŸºæœ¬çŸ¥è¯†ï¼šå·ç§¯ã€é€šé“ã€æ ¸ã€æ± åŒ–(Pooling)ç­‰ã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;é˜…è¯»ææ–™ï¼š&lt;a href=&quot;https://cs231n.github.io/convolutional-networks/&quot;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Lecture 6ä¸»è¦ä»‹ç»æœºå™¨å­¦ä¹ ä¸­çš„ç¡¬ä»¶(CPU, GPU, TPU)å’Œè½¯ä»¶(TensorFlow, PyTorch, Kerasç­‰)ã€‚&lt;/p&gt;

&lt;p&gt;Lecture 7å’Œ8ä¸»è¦è®²è§£æ¨¡å‹è®­ç»ƒä¸­çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚&lt;strong&gt;Batch Normalization&lt;/strong&gt;ç­‰ã€‚è¿™ä¸ªéƒ¨åˆ†æˆ‘æ€»ç»“ä¸€ä¸ªç¬”è®°ï¼š&lt;a href=&quot;https://xietx1995.github.io/2021/optimization&quot;&gt;https://xietx1995.github.io/2021/optimization&lt;/a&gt;ã€‚&lt;/p&gt;

&lt;p&gt;Lecture 9ä¸»è¦ä»‹ç»ä¸»æµçš„ç½‘ç»œæ¶æ„ï¼š&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;AlexNet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;VGGNet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1409.4842&quot;&gt;GoogLeNet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lecture-1015&quot;&gt;Lecture 10~15&lt;/h2&gt;

&lt;p&gt;Lecture 10å’Œ11ä¸»è¦è®²è§£RNNã€Attentionã€Transformerã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RNNé˜…è¯»ææ–™ï¼š&lt;a href=&quot;https://www.deeplearningbook.org/contents/rnn.html&quot;&gt;https://www.deeplearningbook.org/contents/rnn.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;å¦å¤–è¿˜æœ‰ä»¥ä¸‹èµ„æ–™ä¹Ÿå¾ˆä¸é”™ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b&quot;&gt;A simple overview of RNN, LSTM and Attention Mechanism&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21&quot;&gt;Illustrated Guide to LSTMâ€™s and GRUâ€™s: A step by step explanation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;AttentionåŸå§‹è®ºæ–‡ï¼š&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;https://arxiv.org/abs/1409.0473&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Self-AttentionåŸå§‹è®ºæ–‡ï¼š&lt;a href=&quot;https://arxiv.org/abs/1601.06733&quot;&gt;https://arxiv.org/abs/1601.06733&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;Attention? Attention! (lilianweng.github.io)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Attentionè§†é¢‘è®²è§£ï¼š&lt;a href=&quot;https://youtu.be/XhWdv7ghmQQ&quot;&gt;https://youtu.be/XhWdv7ghmQQ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Selft-Attentionè§†é¢‘è®²è§£ï¼š&lt;a href=&quot;https://youtu.be/Vr4UNt7X6Gw&quot;&gt;https://youtu.be/Vr4UNt7X6Gw&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;TransformeråŸå§‹è®ºæ–‡ï¼š&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;https://arxiv.org/abs/1706.03762&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Transformerè§†é¢‘è®²è§£ï¼š&lt;a href=&quot;https://youtu.be/aButdUV0dxI&quot;&gt;https://youtu.be/aButdUV0dxI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer â€“ Jay Alammar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://peterbloem.nl/blog/transformers&quot;&gt;Transformers from scratch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 12å’Œ13åˆ†åˆ«æ˜¯ç”Ÿæˆæ¨¡å‹ã€è‡ªç›‘ç£å­¦ä¹ ï¼Œæå®æ¯…çš„è§†é¢‘è®²å¾—å¾ˆå¥½ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œï¼š&lt;a href=&quot;https://youtu.be/4OWp0wDu6Xw&quot;&gt;https://youtu.be/4OWp0wDu6Xw&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;è‡ªç›‘ç£å­¦ä¹ ï¼š&lt;a href=&quot;https://youtu.be/e422eloJ0W4&quot;&gt;https://youtu.be/e422eloJ0W4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;å…¶ä»–ä¸é”™çš„èµ„æ–™ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/&quot;&gt;A Gentle Introduction to Generative Adversarial Networks (GANs)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;Self-Supervised Representation Learning (Lilian Weng Blog Post)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lecture 14ä¸»è¦è®²ç‰¹å¾å¯è§†åŒ–ï¼Œå¦‚Saliency Mapsç­‰ï¼ŒLecture 15ä¸»è¦è®²ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ç­‰ã€‚&lt;/p&gt;

&lt;h2 id=&quot;lecture-1619&quot;&gt;Lecture 16~19&lt;/h2&gt;

&lt;p&gt;è¿™éƒ¨åˆ†æ˜¯å˜‰å®¾Lectureï¼Œæ— è§†é¢‘ï¼Œéƒ¨åˆ†æ— Slidesã€‚&lt;/p&gt;

&lt;h2 id=&quot;ä½œä¸šéš¾ç‚¹&quot;&gt;ä½œä¸šéš¾ç‚¹&lt;/h2&gt;

&lt;p&gt;CS231nçš„ä½œä¸šæ¯”è¾ƒç¡¬æ ¸ï¼Œå¤§éƒ¨åˆ†å¿…é¡»æ‰‹å†™æ¢¯åº¦è®¡ç®—å’Œåå‘ä¼ æ’­ï¼Œå…¶ä¸­è¿˜æœ‰å¾ˆå¤šæŠ€å·§å’Œç»†èŠ‚ã€‚é€šè¿‡è¿™ç§æŠ˜ç£¨ä¹‹åï¼Œç»“åˆé˜…è¯»æ¡†æ¶éƒ¨åˆ†æºç ï¼Œè‡ªå·±ä¹Ÿå¤§æ¦‚äº†è§£äº†æ¡†æ¶å†…éƒ¨å…·ä½“æ˜¯æ€ä¹ˆåšçš„ã€‚ä¸‹é¢æ€»ç»“ä¸€ä¸‹ä½œä¸šä¸­çš„éš¾ç‚¹ã€‚&lt;/p&gt;

&lt;h3 id=&quot;assignment-1&quot;&gt;Assignment 1&lt;/h3&gt;

&lt;p&gt;ä¸ªäººè®¤ä¸ºä½œä¸š1ä¸­æ¯”è¾ƒéš¾çš„æœ‰ä¸‰ä¸ªï¼Œä¸»è¦æ˜¯éœ€è¦æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Q2: Training a Support Vector Machine&lt;/li&gt;
  &lt;li&gt;Q3: Implement a Softmax classifier&lt;/li&gt;
  &lt;li&gt;Q4: Two-Layer Neural Network&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Q2å’ŒQ3éœ€è¦æ¨å¯¼æ¢¯åº¦è®¡ç®—å…¬å¼ï¼Œå‘é‡åŒ–ç‰ˆæœ¬éœ€è¦æ€è€ƒã€‚å…³äºSoftmaxçš„æ¢¯åº¦è®¡ç®—ï¼Œæˆ‘æ€»ç»“äº†ä¸€ç¯‡æ–‡ç« ï¼š&lt;a href=&quot;https://xietx1995.github.io/2021/softmax&quot;&gt;https://xietx1995.github.io/2021/softmax&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;å…¨è¿æ¥çš„åå‘ä¼ æ’­å°±æ˜¯çŸ©é˜µæ±‚å¯¼ï¼Œå’Œä¸Šæ¸¸æ¢¯åº¦ç›¸ä¹˜å³å¯ã€‚&lt;/p&gt;

&lt;h3 id=&quot;assignment-2&quot;&gt;Assignment 2&lt;/h3&gt;

&lt;p&gt;ä½œä¸š2ä¸­æ¯”è¾ƒéš¾çš„æ˜¯Batch Normalizationå’ŒCNNçš„åå‘ä¼ æ’­ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Q2: Batch Normalization&lt;/li&gt;
  &lt;li&gt;Q4: Convolutional Neural Networks&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ä»¥Batch Normalizationä¸ºä¾‹ï¼Œåªè¦æ¨å‡ºçš„æ¢¯åº¦è®¡ç®—å…¬å¼ï¼Œå†™ä»£ç å°±æ¸…æ™°æ˜äº†ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/CS231n_Summary/bn_computation_graph.jpg&quot; alt=&quot;bn_computation_graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNNçš„åå‘ä¼ æ’­éœ€è¦ä¸€ç‚¹æƒ³è±¡åŠ›ã€‚æ¨èé˜…è¯»æ–‡ç« ï¼š&lt;a href=&quot;https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c&quot;&gt;Convolutions and Backpropagations&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;assignment-3&quot;&gt;Assignment 3&lt;/h3&gt;

&lt;p&gt;ä½œä¸š3ä¸­æ¯”è¾ƒéš¾çš„æ˜¯RNNå’ŒTransformerçš„æ¢¯åº¦è®¡ç®—ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Q1: Image Captioning with Vanilla RNNs&lt;/li&gt;
  &lt;li&gt;Q2: Image Captioning with Transformers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RNNä¸­æ¯”è¾ƒéš¾çš„æ˜¯æ¢¯åº¦åœ¨æ—¶é—´æ­¥ä¹‹é—´çš„ç§¯ç´¯ï¼ŒTransformerä¸­æ¯”è¾ƒéš¾çš„æ˜¯ç†è§£Qã€Kã€VçŸ©é˜µçš„åå‘ä¼ æ’­ã€‚&lt;/p&gt;

&lt;p&gt;æ€»ä¹‹ï¼Œåˆ©ç”¨è®¡ç®—å›¾ï¼Œå°±å¯ä»¥å¾ˆæ–¹ä¾¿åœ°æ¨å¯¼å‡ºæ¢¯åº¦è®¡ç®—å…¬å¼ï¼Œå†™å‡ºä»£ç å°±æ¯”è¾ƒé¡ºç†æˆç« äº†ï¼Œä½†æ˜¯å¦‚æœè¦å†™å‡ºå‘é‡åŒ–çš„ä»£ç è¿˜æ˜¯æ¯”è¾ƒéš¾çš„ï¼Œè¿™éœ€è¦å¯¹numpyã€çŸ©é˜µè®¡ç®—æœ‰æ›´è¿›ä¸€æ­¥çš„æŒæ¡æ‰è¡Œã€‚&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;æ€»ä¹‹ï¼ŒCS231nè¿˜åªæ˜¯èµ·æ­¥ï¼Œæƒ³è¦ç ”ç©¶å„ä¸ªç»†åˆ†çš„æ–¹å‘éœ€è¦æ›´è¿›ä¸€æ­¥çš„å­¦ä¹ æ‰èƒ½è¿›æ­¥ã€‚&lt;/p&gt;</content><author><name>Tianxin Xie</name></author><category term="MachineLearning" /><category term="MachineLearning" /><summary type="html">æ–­æ–­ç»­ç»­èŠ±äº†å¤§çº¦äº”å‘¨çš„æ—¶é—´å­¦å®Œäº† CS231nï¼Œæ˜¯æ—¶å€™æ€»ç»“ä¸€ä¸‹è‡ªå·±é‡åˆ°çš„é—®é¢˜å’Œæ”¶è·äº†ã€‚</summary></entry><entry><title type="html">æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“</title><link href="https://xietx1995.github.io/2021/optimization" rel="alternate" type="text/html" title="æœºå™¨å­¦ä¹ ç¬”è®° - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•æ€»ç»“" /><published>2021-07-04T00:00:00+08:00</published><updated>2021-07-04T00:00:00+08:00</updated><id>https://xietx1995.github.io/2021/MLNotes_Optimization</id><content type="html" xml:base="https://xietx1995.github.io/2021/optimization">&lt;p&gt;åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ€§èƒ½ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒä¸€ç¯‡ &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/&quot;&gt;review&lt;/a&gt;ï¼Œæ€»ç»“äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç¿»è¯‘è‡ªè¯¥ç¯‡ &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/&quot;&gt;review&lt;/a&gt;ï¼Œåšäº†å¢åˆ æ”¹ï¼Œå¹¶åŠ ä¸Šäº†éƒ¨åˆ†è‡ªå·±çš„ç†è§£ï¼Œä»¥åŠé¢å¤–çš„å›¾ç¤ºã€è§†é¢‘å’Œé˜…è¯»ææ–™ã€‚&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This blog post aims at providing you with intuitions towards the behaviour of different algorithms for optimizing gradient descent that will help you put them to use. We are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training. Subsequently, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules. We will also take a short look at algorithms and architectures to optimize gradient descent in a parallel and distributed setting. Finally, we will consider additional strategies that are helpful for optimizing gradient descent.&lt;/p&gt;

  &lt;p&gt;â€‹                                                                                              â€“  â€œAn overview of gradient descent optimization algorithmsâ€&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;1-æ¢¯åº¦ä¸‹é™çš„å˜ç§&quot;&gt;1 æ¢¯åº¦ä¸‹é™çš„å˜ç§&lt;/h2&gt;

&lt;p&gt;è¿™é‡Œä»‹ç»ä¸‰ç§æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch Gradient Descent&lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent&lt;/li&gt;
  &lt;li&gt;Mini-batch gradient descent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;è¿™ä¸‰ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•çš„å”¯ä¸€åŒºåˆ«å°±æ˜¯æ¯ä¸ª step ä½¿ç”¨çš„æ•°æ®é‡ä¸åŒã€‚&lt;/p&gt;

&lt;h3 id=&quot;11-batch-gradient-descent&quot;&gt;1.1 Batch Gradient Descent&lt;/h3&gt;

&lt;p&gt;è¿™æ˜¯æœ€åŸå§‹çš„æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œç¿»è¯‘è¿‡æ¥å°±æ˜¯â€æ‰¹é‡æ¢¯åº¦ä¸‹é™â€œï¼Œæ¯ä¸ª step ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†ã€‚&lt;/p&gt;

\[\theta = \theta - \eta\cdot\triangledown_{\theta} J(\theta)\]

&lt;p&gt;è¿™ä¸ªæ–¹æ³•æœ‰ä¸‰ä¸ªé—®é¢˜ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;æ¯ä¸ª step éƒ½éœ€è¦å¯¹æ•´ä¸ªè®­ç»ƒé›†è®¡ç®—æ¢¯åº¦ï¼Œé€Ÿåº¦å¾ˆæ…¢ï¼›&lt;/li&gt;
  &lt;li&gt;ç°åœ¨çš„æ•°æ®é›†éƒ½è¾ƒå¤§ï¼Œå†…å­˜è£…ä¸ä¸‹æ•´ä¸ªè®­ç»ƒé›†ï¼›&lt;/li&gt;
  &lt;li&gt;ä¸èƒ½è¿›è¡Œ online trainingï¼Œä¾‹å¦‚å·²ç»éƒ¨ç½²å¥½çš„ä¸€ä¸ªæ¨¡å‹ï¼Œä¸èƒ½ç”¨æ–°æ ·æœ¬è°ƒæ•´æ¨¡å‹ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Batch Gradient Descent çš„ä»£ç å½¢å¼é€šå¸¸å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;é¦–å…ˆè®¡ç®—å‚æ•°å…³äºæ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦ï¼Œç„¶åå†ç”¨æ¢¯åº¦ä¹˜ä»¥å­¦ä¹ ç‡æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚å¦‚æœ loss å‡½æ•°æ˜¯å‡¸é¢ï¼Œæ‰¹é‡æ¢¯åº¦ä¸‹é™èƒ½ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜çš„å‚æ•°ï¼Œå¦åˆ™æ‰¾åˆ°çš„å¯èƒ½æ˜¯å±€éƒ¨æœ€ä¼˜å€¼ã€‚&lt;/p&gt;

&lt;h3 id=&quot;12-stochastic-gradient-descent&quot;&gt;1.2 Stochastic Gradient Descent&lt;/h3&gt;

&lt;p&gt;SGD ä¹Ÿç§°ä¸ºéšæœºæ¢¯åº¦ä¸‹é™ï¼Œå’Œ BGD æ˜¯ä¸¤ä¸ªæç«¯ï¼ŒBGD ä¸€ä¸ª step ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†ï¼ŒSGD ä¸€æ¬¡åªç”¨ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ $(x^{(i)}, y^{(i)})$ ï¼š&lt;/p&gt;

\[\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})\]

&lt;p&gt;SGDæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;é€Ÿåº¦å¿«ï¼Œæ¯æ¬¡åªç”¨ä¸€ä¸ªæ ·æœ¬ï¼›&lt;/li&gt;
  &lt;li&gt;å¯ä»¥è¿›è¡Œ online trainingã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SGD ä¹Ÿæœ‰ä¸€ä¸ªæ¯”è¾ƒå¤§çš„ç¼ºç‚¹ï¼Œå°±æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œloss å‡½æ•°çš„æ³¢åŠ¨å¾ˆå¤§ï¼Œå¯¼è‡´è¾ƒé«˜çš„ varianceï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Optimization.assets/sgd_fluctuation.png&quot; alt=&quot;sgd_fluctuation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;å¾ˆå®¹æ˜“å°±èƒ½æƒ³è±¡ï¼ŒSGD çš„è¿™ç§æ³¢åŠ¨ä½¿å¾—æ¨¡å‹å®¹æ˜“è¶Šè¿‡æœ€ä¼˜ç‚¹ã€‚ä½†æ˜¯å®éªŒè¡¨æ˜ï¼Œå¦‚æœåœ¨æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä¸­é€æ¸å‡å°å­¦ä¹ ç‡ï¼Œæœ€ç»ˆå‡ ä¹å’Œ BGD ä¸€æ ·èƒ½å¤Ÿåˆ°è¾¾æœ€ä¼˜ç‚¹ (convex loss) æˆ–è€…å±€éƒ¨æœ€ä¼˜ç‚¹ (non-convex loss)ã€‚ä¼ªä»£ç å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;æ¯ä¸ª epoch éƒ½å¯¹æ•°æ®é›†è¿›è¡Œäº† &lt;a href=&quot;https://stats.stackexchange.com/a/311318&quot;&gt;shuffle&lt;/a&gt;ï¼Œç„¶åæ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬æ›´æ–°å‚æ•°ã€‚&lt;/p&gt;

&lt;h3 id=&quot;13-mini-batch-gradient-descent&quot;&gt;1.3 Mini-batch Gradient Descent&lt;/h3&gt;

&lt;p&gt;ä¸­æ–‡ç§°å…¶ä¸ºå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼Œæˆ‘ä»¬å°±ç®€ç§° MBGD å§ã€‚&lt;/p&gt;

\[\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})\]

&lt;p&gt;MBGD æ¯ä¸ª step åªä½¿ç”¨è®­ç»ƒé›†çš„ä¸€å°éƒ¨åˆ†ï¼Œæ ¹æ®æ•°æ®é›†å¤§å°ã€è®¾å¤‡æ€§èƒ½ã€å†…å­˜ã€æ˜¾å­˜å®¹é‡ç­‰ï¼Œä¸€èˆ¬å– 32ï¼Œ 64ï¼Œ 128ï¼Œ 256ï¼Œ 512 ç­‰ã€‚å®ƒç»¼åˆäº† BGD å’Œ SGD çš„ä¼˜ç‚¹ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;loss å’Œå‚æ•°ä¸ä¼šæœ‰å¾ˆå¤§çš„æ³¢åŠ¨ï¼›&lt;/li&gt;
  &lt;li&gt;è®­ç»ƒé€Ÿåº¦å¿« (ç›®å‰çš„è®­ç»ƒæ¡†æ¶èƒ½é«˜æ•ˆåœ°è®¡ç®—æ¢¯åº¦)ï¼›&lt;/li&gt;
  &lt;li&gt;å¯ä»¥ online training (åªéœ€é›†é½ä¸€ä¸ªbatchå°±å¯ä»¥)ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MBGD ä¹Ÿèƒ½è¾¾æœ€ä¼˜ç‚¹ (convex loss) æˆ–è€…å±€éƒ¨æœ€ä¼˜ç‚¹ (non-convex loss)ã€‚ä¼ªä»£ç ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# æ¯ä¸ªstepå–50ä¸ªæ ·æœ¬
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-é—®é¢˜æŒ‘æˆ˜&quot;&gt;2 é—®é¢˜æŒ‘æˆ˜&lt;/h2&gt;

&lt;p&gt;Mini-batch gradient descent è™½ç„¶ä¸€å®šç¨‹åº¦ä¸Šå…‹æœäº† BGD å’Œ SGD çš„ä¸€äº›ç¼ºç‚¹ï¼Œä½†æ˜¯è¿˜é¢ä¸´å¦‚ä¸‹é—®é¢˜ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;éš¾ä»¥é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ï¼Œè¿‡å°å¯¼è‡´è®­ç»ƒé€Ÿåº¦æ…¢ï¼Œè¿‡å¤§å¯¼è‡´ loss æ³¢åŠ¨ç”šè‡³å‘æ•£ï¼›&lt;/li&gt;
  &lt;li&gt;å¯ä»¥å¯¹å­¦ä¹ ç‡åŠ¨æ€è°ƒæ•´ï¼Œå¦‚è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å‡å°å­¦ä¹ ç‡ï¼Œä½†æ˜¯å¦‚ä½•å‡å°ï¼Œå‡å°å¤šå°‘ä¹Ÿæ˜¯éœ€è¦æå‰å®šä¹‰çš„ã€‚è€Œæ•°æ®é›†çš„ä¸åŒå¯èƒ½éœ€è¦ä¸åŒçš„è°ƒæ•´ç­–ç•¥ï¼›&lt;/li&gt;
  &lt;li&gt;åŒä¸€ä¸ªå­¦ä¹ ç‡å¯¹ä¸åŒå‚æ•°çš„æ•ˆæœå¯èƒ½å¤§ç›¸å¾„åº­ï¼Œå¦‚æœæ•°æ®é›†å„ä¸ªç‰¹å¾çš„é¢‘ç‡ (å‡ºç°æ¬¡æ•°) å·®å¼‚è¾ƒå¤§ï¼Œæˆ‘ä»¬å¯èƒ½ä¸æƒ³å¯¹å®ƒä»¬é‡‡å–åŒæ ·çš„æ›´æ–°ç­–ç•¥ã€‚ä¾‹å¦‚é¢‘ç‡è¾ƒå°çš„ç‰¹å¾ï¼Œå¯èƒ½éœ€è¦è¾ƒå¤§çš„å­¦ä¹ ç‡ï¼›&lt;/li&gt;
  &lt;li&gt;ä¸€ä¸ªè¾ƒå¤§çš„é—®é¢˜æ˜¯ç›®å‰å¾ˆå¤šæ¨¡å‹ (ä½¿ç”¨ç¥ç»ç½‘ç»œ) éƒ½æ˜¯ non-convex çš„ï¼Œæ‰€ä»¥æ¨¡å‹å¾ˆå¯èƒ½è¢«è®­ç»ƒåˆ°ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜ç‚¹ã€‚å¦å¤–ï¼Œæ¨¡å‹å¦‚æœè½å…¥&lt;a href=&quot;https://en.wikipedia.org/wiki/Saddle_point&quot;&gt;éç‚¹&lt;/a&gt; (saddle points)ï¼Œå°±å¾ˆéš¾ç»§ç»­è®­ç»ƒï¼Œå› ä¸ºå„ä¸ªæ–¹å‘çš„æ¢¯åº¦éƒ½å‡ ä¹ä¸º 0ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•&quot;&gt;3 æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•&lt;/h2&gt;

&lt;p&gt;æ¥ä¸‹æ¥ä»‹ç»å®è·µä¸­å¸¸ç”¨äºåº”å¯¹ä¸Šè¿°é—®é¢˜çš„æ–¹æ³• (ä¸åŒ…æ‹¬å¯¹é«˜ç»´æ•°æ®è®¡ç®—å¤æ‚åº¦è¾ƒå¤§çš„æ–¹æ³•ï¼Œå¦‚äºŒé˜¶ç‰›é¡¿æ³•)ã€‚&lt;/p&gt;

&lt;h3 id=&quot;31-momentum&quot;&gt;3.1 Momentum&lt;/h3&gt;

&lt;p&gt;Momentum åˆç§°ä¸ºåŠ¨é‡æ³•ï¼Œå…¶æ€è·¯å¾ˆç®€å•ã€‚æƒ³è±¡ä¸€ä¸ªå°çƒä»é«˜å¤„æ»šè½ï¼Œå°çƒçš„é€Ÿåº¦æ˜¯çŸ¢é‡ï¼Œå¯ä»¥åˆ†è§£ä¸ºå„ä¸ªæ–¹å‘çš„åˆ†é‡ï¼Œä¸€ç›´ä¸‹é™çš„æ–¹å‘çš„é€Ÿåº¦åˆ†é‡ä¼šè¶Šæ¥è¶Šå¤§ï¼Œè€Œæœ‰ä¸‹é™åˆæœ‰ä¸Šå‡çš„æ–¹å‘ï¼Œé€Ÿåº¦åˆ†é‡ä¼šç¼“æ…¢å¢åŠ ã€ç”šè‡³å‡å°‘ã€‚ä»ä¸‹é¢çš„ç¤ºæ„å›¾ä¸­å¯ä»¥çœ‹åˆ° momentum æ–¹æ³•é€Ÿåº¦éå¸¸å¿«ã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Optimization.assets/sgd_comparison.gif&quot; alt=&quot;GitHub - ilguyi/optimizers.numpy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;åœ¨ SGD with moment æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ç»´æŒä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯å‘é‡ $v$ (ç±»æ¯”å°çƒé€Ÿåº¦çŸ¢é‡)ï¼Œå…¬å¼å¦‚ä¸‹ï¼š&lt;/p&gt;

\[\begin{align}
\begin{split}
v_t &amp;amp;= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\
\theta &amp;amp;= \theta - v_t
\end{split}
\end{align}\]

&lt;p&gt;æœ‰äº›æ–‡ç« ä¸­çš„ç¬¦å·æ˜¯ç›¸åçš„ï¼Œä¹Ÿæ˜¯æ­£ç¡®çš„ï¼š&lt;/p&gt;

\[\begin{align}
\begin{split}
v_t &amp;amp;= \gamma v_{t-1} - \eta \nabla_\theta J( \theta) \\
\theta &amp;amp;= \theta + v_t
\end{split}
\end{align}\]

&lt;p&gt;$\gamma$ æ˜¯ momentum çš„æƒé‡ï¼Œä¸€èˆ¬å– 0.9ã€‚&lt;/p&gt;

&lt;h3 id=&quot;32-nesterov-accelerated-gradient&quot;&gt;3.2 â€‹Nesterov Accelerated Gradient&lt;/h3&gt;

&lt;p&gt;Nesterov Accelerated Gradient(NAG) å¯¹ momentum åšäº†ä¸€ç‚¹æ”¹è¿›ï¼š&lt;/p&gt;

\[\begin{align}
\begin{split}
v_t &amp;amp;= \gamma v_{t-1} + \eta \nabla_\theta J(\theta-\gamma v_{t-1}) \\
\theta &amp;amp;= \theta - v_t
\end{split}
\end{align}\]

&lt;p&gt;NAG å…ˆå¾€å‰è¿ˆä¸€æ­¥ï¼Œå³ $\theta-\gamma v_{t-1}$ï¼Œç„¶åè®¡ç®—æ¢¯åº¦ï¼Œæ¥ç€ç”¨è®¡ç®—å‡ºçš„æ¢¯åº¦ä¿®æ­£å½“å‰çš„æ¢¯åº¦ç´¯ç§¯é‡ã€‚è€Œ momentum æ˜¯å…ˆåœ¨å½“å‰ä½ç½®è®¡ç®—æ¢¯åº¦ï¼Œç„¶åå†è·³ä¸€å¤§æ­¥æ›´æ–°ä½ç½®ã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Optimization.assets/nesterov.png&quot; alt=&quot;nesterov&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ä¸Šå›¾ä¸­ï¼Œç»¿è‰²ä¸ºæœ€ä½³è·¯çº¿ï¼Œè“è‰²ä¸º momentum è·¯çº¿ (å…ˆè®¡ç®—æ¢¯åº¦ï¼Œç´¯è®¡ï¼Œå†è·³ä¸€å¤§æ­¥)ï¼Œè¤è‰² + çº¢è‰²ä¸º NAG è·¯çº¿ (å…ˆè·³ä¸€å¤§æ­¥ï¼Œå†è®¡ç®—æ¢¯åº¦ï¼Œä¿®æ­£)ã€‚&lt;/p&gt;

&lt;h3 id=&quot;33-adagrad&quot;&gt;3.3 AdaGrad&lt;/h3&gt;

&lt;p&gt;â€AdaGradâ€ â€å¯ä½¿å­¦ä¹ é€Ÿç‡é€‚åº”å‚æ•°ã€‚å®ƒå¯¹æ ·æœ¬ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„ç‰¹å¾å¯¹åº”çš„å‚æ•°æ‰§è¡Œè¾ƒå°çš„æ›´æ–°ï¼Œå¯¹é¢‘ç‡è¾ƒä½çš„ç‰¹å¾å¯¹åº”çš„å‚æ•°æ‰§è¡Œè¾ƒå¤§çš„æ›´æ–°ã€‚å…¶æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;åœ¨æ­¥éª¤ tï¼Œç”¨ $s_t$ ä¿å­˜å„ä¸ªå‚æ•°æ¯ä¸ª step çš„æ¢¯åº¦çš„å¹³æ–¹å’Œï¼š&lt;/p&gt;

\[s_t = s_{t-1} + g_t\odot g_t\]

    &lt;p&gt;è¿™é‡Œ $\odot$ æ˜¯å…ƒç´ å¯¹åº”ç›¸ä¹˜ï¼Œå³è®¡ç®—æ¢¯åº¦å‘é‡æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹ã€‚å¯ä»¥çœ‹å‡ºï¼Œå¦‚æœæŸä¸ªå‚æ•°ä¸‹é™å¾ˆå¿«ï¼Œé‚£ä¹ˆ $s_t$ å¯¹åº”åˆ†é‡çš„ç´¯è®¡å°±è¶Šå¤§ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æ›´æ–°å‚æ•°ï¼š&lt;/p&gt;

\[\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t\]

    &lt;p&gt;å› ä¸ºä¸‹é™è¶Šå¿«çš„å‚æ•° $s_t$ ä¸­çš„åˆ†é‡è¶Šå¤§ï¼Œé‚£ä¹ˆ $\frac{\eta}{\sqrt{s_t + \epsilon}}$ å°±ä¼šè¶Šå°ï¼Œå³å­¦ä¹ é€Ÿç‡å°±æ›´å°ï¼Œåä¹‹ä¸‹é™æ…¢çš„å‚æ•°ï¼Œå¯¹åº”çš„å­¦ä¹ é€Ÿç‡å°±é«˜ä¸€ç‚¹ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;è¿™é‡Œçš„ $\epsilon$ æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ­£å€¼ï¼Œé˜²æ­¢è®¡ç®—è¿‡ç¨‹ä¸­é™¤é›¶ (é€šå¸¸å– $1e-8$ çº§åˆ«çš„æ•°)ã€‚å¦å¤–ä¸€ç‚¹æ˜¯ä¸å¼€æ–¹çš„è¯ï¼Œæ•ˆæœä¼šå¾ˆå·®ã€‚&lt;/p&gt;

&lt;p&gt;AdaGrad çš„å¥½å¤„æ˜¯ä¸éœ€è¦æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡ã€‚ä½†æœ‰ä¸ªç¼ºç‚¹ï¼šç”±äºç´¯ç§¯æ¢¯åº¦å‘é‡ $s_t$ çš„å…ƒç´ æ¯ä¸ªæ­¥éª¤æ–°å¢é¡¹éƒ½æ˜¯æ­£æ•° (è¯¥åˆ†é‡æ¢¯åº¦çš„å¹³æ–¹)ï¼Œå› æ­¤åœ¨è®­ç»ƒæœŸé—´ç´¯ç§¯çš„æ€»å’Œä¸æ–­å¢åŠ ï¼Œ $\frac{\eta}{\sqrt{s_t + \epsilon}}$ ä¸æ–­å‡å°ï¼Œå¯¼è‡´å­¦ä¹ ç‡ç¼©å°å¹¶å˜å¾—æå° (å­¦ä¹ ç‡æ¶ˆå¤±)â€ã€‚&lt;/p&gt;

&lt;p&gt;AdaGrad äº 2011 å¹´è¢«æå‡ºï¼Œç”±äºå­¦ä¹ ç‡æ¶ˆå¤±çš„é—®é¢˜ï¼Œå·²ç»åŸºæœ¬æ— äººé‡‡ç”¨ï¼Œå…·ä½“ç»†èŠ‚å¯ä»¥é˜…è¯»è®ºæ–‡ â€Duchi, J., Hazan, E., &amp;amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121â€“2159.â€œã€‚&lt;/p&gt;

&lt;h3 id=&quot;34-adadelta&quot;&gt;3.4 AdaDelta&lt;/h3&gt;

&lt;p&gt;AdaDelta æ˜¯ â€AdaGradâ€ çš„æ”¹è‰¯ç‰ˆæœ¬ï¼Œç›®çš„æ˜¯è§£å†³æ¢¯åº¦å¹³æ–¹ç´¯è®¡å¯¼è‡´çš„å­¦ä¹ é€Ÿç‡æå°(æ¶ˆå¤±)çš„é—®é¢˜ã€‚å…¶æ€è·¯å¾ˆç®€å•ï¼Œç»´æŠ¤ä¸€ä¸ªç´¯ç§¯çª—å£ï¼Œè€Œä¸æ˜¯ç´¯ç§¯æ¯ä¸€æ­¥çš„æ¢¯åº¦ã€‚å…·ä½“å‚è€ƒï¼š&lt;a href=&quot;https://paperswithcode.com/method/adadelta&quot;&gt;AdaDelta Explained&lt;/a&gt;ã€‚&lt;/p&gt;

&lt;p&gt;å››ç§æ–¹æ³•æ¯”è¾ƒï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Optimization.assets/sgd_adagrad.png&quot; alt=&quot;sgd_adagrad&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;35-rmsprop&quot;&gt;3.5 RMSProp&lt;/h3&gt;

&lt;p&gt;RMSprop æœªç»è®ºæ–‡å‘è¡¨ï¼Œæ˜¯ä¸€ç§é€‚åº”æ€§çš„å­¦ä¹ é€Ÿç‡è°ƒæ•´æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”± Geoff Hinton åœ¨ä»–çš„ Coursera &lt;a href=&quot;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&quot;&gt;è¯¾ç¨‹è®²ä¹‰&lt;/a&gt;ä¸­æå‡ºï¼Œå’Œ AdaDelta çš„ç›®çš„ç›¸åŒï¼Œä¹Ÿæ˜¯ä¸ºäº†è§£å†³ AdaGrad å­¦ä¹ ç‡æ¶ˆå¤±é—®é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;RMSProp çš„æ›´æ–°è§„åˆ™å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;åœ¨æ­¥éª¤ tï¼Œè®¡ç®—ï¼š&lt;/p&gt;

\[s_t = \gamma s_{t-1} + (1-\gamma)g_t\odot g_t\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;æ›´æ–°å‚æ•°ï¼š&lt;/p&gt;

\[\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{s_t + \epsilon}} \odot g_t\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;æˆ‘ä»¬æŠŠç¬¬ä¸€æ­¥è®¡ç®— $s_t$ çš„å…¬å¼å±•å¼€ï¼š&lt;/p&gt;

\[\begin{split}\begin{aligned}
\mathbf{s}_t &amp;amp; = (1 - \gamma) \mathbf{g}_t^2 + \gamma \mathbf{s}_{t-1} \\
&amp;amp; = (1 - \gamma) \left(\mathbf{g}_t^2 + \gamma \mathbf{g}_{t-1}^2 + \gamma^2 \mathbf{g}_{t-2} + \ldots, \right).
\end{aligned}\end{split}\]

&lt;p&gt;æˆ‘ä»¬çŸ¥é“ $1 + \gamma + \gamma^2 + \ldots = \frac{1}{1-\gamma}$ï¼Œå³ $s_t$ æ˜¯ä¹‹å‰æ­¥éª¤ç´¯ç§¯æ¢¯åº¦çš„æœŸæœ›å€¼ï¼Œå› ä¸º $\gamma &amp;lt; 1$ï¼Œè¶Šæ—©çš„æ¢¯åº¦ç´¯è®¡å¯¹å½“å‰çš„å½±å“è¶Šå°ï¼Œå¾ˆå¤šæ­¥ä¹‹å‰çš„æ¢¯åº¦ç´¯è®¡ç”šè‡³å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œæ•…ä¸ä¼šå‡ºç° $s_t$ æ— é™åˆ¶å¢é•¿ï¼Œå¯¼è‡´å­¦ä¹ ç‡æ¶ˆå¤±çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ $\gamma$ æ¥æ§åˆ¶ $s_t$ å¯¹å†å²æ¢¯åº¦çš„ç´¯ç§¯èŒƒå›´ã€‚ä¸‹é¢çš„å›¾ç‰‡æ˜¯ä¸åŒ $\gamma$ å–å€¼å¯¹åº”çš„å†å²æ¢¯åº¦ç´¯è®¡çš„æƒé‡ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Optimization.assets/rmsprop_gamma.png&quot; alt=&quot;rmsprop_gamma&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œå–å€¼è¶Šå¤§ï¼Œ$s_t$ è¶Šâ€å…³æ³¨â€œå†å²ç§¯ç´¯çš„æ¢¯åº¦ã€‚40 æ­¥ä¹‹å‰çš„ç§¯ç´¯ï¼Œè™½ç„¶åœ¨è§£æå¼ä¸­ä¸ä¸º 0ï¼Œä½†åœ¨å†…å­˜ä¸­æ•°å€¼ä¸Šå·²ç»ä¸º 0ã€‚&lt;/p&gt;

&lt;h3 id=&quot;36-adam&quot;&gt;3.6 Adam&lt;/h3&gt;

&lt;p&gt;Adam æ˜¯ RMSProp å’Œ Momentum çš„ç»“åˆã€‚&lt;/p&gt;

\[\begin{split}\begin{aligned}
    \mathbf{v}_t &amp;amp; \leftarrow \beta_1 \mathbf{v}_{t-1} + (1 - \beta_1) \mathbf{g}_t \\
    \mathbf{s}_t &amp;amp; \leftarrow \beta_2 \mathbf{s}_{t-1} + (1 - \beta_2) \mathbf{g}_t \odot \mathbf{g}_t
\end{aligned}\end{split}\]

&lt;p&gt;ç¬¬ä¸€è¡Œå…¬å¼å³ Momentumï¼Œé€šå¸¸ $\beta_1 = 0.9$ã€‚ç¬¬äºŒè¡Œå³ RMSPropï¼Œé€šå¸¸ $\beta_2 = 0.999$ã€‚&lt;/p&gt;

&lt;p&gt;åœ¨åˆšå¼€å§‹æ—¶æˆ‘ä»¬æŠŠ $\mathbf{v}_t$ å’Œ $\mathbf{s}_t$ è®¾ä¸º 0ï¼Œå³ $\mathbf{v}_0 = \mathbf{s}_0 = 0$ï¼Œé‚£ä¹ˆæœ‰ $\mathbf{v}_1 = 0.1 \mathbf{g}_1$ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„æ¢¯åº¦å€¼ã€‚$\mathbf{s}_1$ ä¹Ÿæœ‰åŒæ ·çš„é—®é¢˜ã€‚æ•…è¶Šæ—©çš„æ—¶é—´æ­¥ï¼Œåå·®è¶Šå¤§ï¼Œå› æ­¤æˆ‘ä»¬è¦ä¿®æ­£åå·®ã€‚&lt;/p&gt;

&lt;p&gt;åœ¨æ—¶é—´æ­¥ t æˆ‘ä»¬å¾—åˆ°&lt;/p&gt;

\[\boldsymbol{v}_t = (1-\beta_1) \sum_{i=1}^{t} \beta_1^{t-i} \boldsymbol{g}_i\]

&lt;p&gt;(ä¸Šé¢ç¬¬ä¸€ä¸ªå¼å­å±•å¼€)ï¼Œå°†è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦çš„æƒå€¼ç›¸åŠ ï¼Œå¾—åˆ°&lt;/p&gt;

\[(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t\]

&lt;p&gt;éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“ t è¾ƒå°æ—¶ï¼Œè¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¼šè¾ƒå°ã€‚ä¾‹å¦‚ï¼Œå½“ $\beta_1 = 0.9$ æ—¶ï¼Œ$\mathbf{v}_1 = 0.1 \mathbf{g}_1$ï¼Œæ‰€æœ‰æƒå€¼ä¹‹å’Œä¸º 0.1ã€‚ä¸ºäº†æ¶ˆé™¤è¿™æ ·çš„å½±å“ï¼Œå¯¹äºä»»æ„æ—¶é—´æ­¥ tï¼Œæˆ‘ä»¬å¯ä»¥å°† $\mathbf{v}_t$ å†é™¤ä»¥$1 - \beta_1^t$ï¼Œä»è€Œä½¿è¿‡å»å„æ—¶é—´æ­¥å°æ‰¹é‡éšæœºæ¢¯åº¦æƒå€¼ä¹‹å’Œä¸º1ã€‚è¿™ä¹Ÿå«ä½œåå·®ä¿®æ­£ã€‚åœ¨Adamç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å¯¹å˜é‡å‡ $\mathbf{v}_t$ å’Œ $\mathbf{s}_t$ ä½œåå·®ä¿®æ­£ï¼š&lt;/p&gt;

\[\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t},\]

\[\hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}.\]

&lt;p&gt;æœ€åè¿›è¡Œæ›´æ–°ï¼š&lt;/p&gt;

\[\boldsymbol{x}_t \leftarrow
\boldsymbol{x}_t - \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon},\]

&lt;p&gt;å…¶ä¸­ $\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œ$\epsilon$ æ˜¯ä¸ºäº†ç»´æŒæ•°å€¼ç¨³å®šæ€§è€Œæ·»åŠ çš„å¸¸æ•°ï¼Œå¦‚ $10^{âˆ’8}$ã€‚å’Œ AdaGrad ç®—æ³•ã€RMSProp ç®—æ³•ä»¥åŠAdaDelta ç®—æ³•ä¸€æ ·ï¼Œç›®æ ‡å‡½æ•°è‡ªå˜é‡ä¸­æ¯ä¸ªå…ƒç´ éƒ½åˆ†åˆ«æ‹¥æœ‰è‡ªå·±çš„å­¦ä¹ ç‡ã€‚&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ğŸ‘å¼ºçƒˆæ¨èä»¥ä¸‹è§†é¢‘ï¼Œè®²äº†Momentum, Adagrad, RMSProp, Adam çš„åŸç†å’Œæ¼”è¿›å…³ç³»ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bç«™(æ— å­—å¹•)ï¼š&lt;a href=&quot;https://www.bilibili.com/video/BV134411q793?share_source=copy_web&quot;&gt;https://www.bilibili.com/video/BV134411q793?share_source=copy_web&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;æ²¹ç®¡(æœ‰è‡ªåŠ¨ç”Ÿæˆå­—å¹•)ï¼š&lt;a href=&quot;https://www.youtube.com/watch?v=gmwxUy7NYpA&quot;&gt;L26/1 Momentum, Adagrad, RMSProp, Adam - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;37-å…¶ä»–ä¼˜åŒ–å™¨&quot;&gt;3.7 å…¶ä»–ä¼˜åŒ–å™¨&lt;/h3&gt;

&lt;p&gt;è¯¥ç¯‡ &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/&quot;&gt;review&lt;/a&gt; è¿˜è®²è§£äº†å…¶ä»–ä¼˜åŒ–å™¨ï¼Œç”±äºåœ¨å®è·µä¸­ï¼Œé™¤éæ˜¯ä¸“é—¨ç ”ç©¶ä¼˜åŒ–å™¨ï¼ŒåŸºæœ¬ä¸ä¼šä½¿ç”¨å®ƒä»¬ï¼Œæ‰€ä»¥æœ¬æ–‡èŠ‚çº¦æ—¶é—´ç‚¹åˆ°ä¸ºæ­¢ã€‚è¿™äº›ä¼˜åŒ–å™¨å‡åœ¨ &lt;a href=&quot;https://paperswithcode.com/methods/category/stochastic-optimization&quot;&gt;An Overview of Stochastic Optimization&lt;/a&gt; åˆ—å‡ºï¼Œæ„Ÿå…´è¶£å¯ä»¥æŸ¥çœ‹ã€‚ä¸‹é¢æ˜¯å„ç§ä¼˜åŒ–å™¨åœ¨éç‚¹ä¸Šçš„è¡¨ç°ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Optimization.assets/saddle_point_evaluation_optimizers.gif&quot; alt=&quot;saddle_point_evaluation_optimizers&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;38-é€‰ç”¨å“ªä¸ªä¼˜åŒ–å™¨&quot;&gt;3.8 é€‰ç”¨å“ªä¸ªä¼˜åŒ–å™¨ï¼Ÿ&lt;/h3&gt;

&lt;p&gt;è¯¥ç¯‡ &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/&quot;&gt;review&lt;/a&gt; ä¸­å†™é“ï¼š&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you wonâ€™t need to tune the learning rate but likely achieve the best results with the default value.&lt;/p&gt;

  &lt;p&gt;In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [&lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fn14&quot;&gt;14:1]&lt;/a&gt; show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, ==Adam might be the best overall choice==.&lt;/p&gt;

  &lt;p&gt;Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.&lt;/p&gt;

  &lt;p&gt;â€‹                                                                                                  â€“  â€œAn overview of gradient descent optimization algorithmsâ€&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;å¤§å¤šæ•°æƒ…å†µç”¨ Adam å°±å¯¹äº†ï¼&lt;/p&gt;

&lt;h2 id=&quot;4-å…¶ä»–ä¼˜åŒ–ç­–ç•¥&quot;&gt;4 å…¶ä»–ä¼˜åŒ–ç­–ç•¥&lt;/h2&gt;

&lt;p&gt;è¯¥ç¯‡ &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#additionalstrategiesforoptimizingsgd&quot;&gt;review&lt;/a&gt; ä¸­æåˆ°çš„å…³äº SGD çš„å…¶ä»–ä¼˜åŒ–ç­–ç•¥åŒ…æ‹¬ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network&quot;&gt;Shuffling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/&quot;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/&quot;&gt;Early Stopping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06807&quot;&gt;Gradient Noise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Shuffling å’Œ Batch Normalization æ˜¯å®è·µä¸­å¸¸ç”¨çš„ã€‚&lt;/p&gt;

&lt;h2 id=&quot;æ¨èé˜…è¯»&quot;&gt;æ¨èé˜…è¯»&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html&quot;&gt;An overview of gradient descent optimization algorithms (ruder.io)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://paperswithcode.com/methods/category/stochastic-optimization&quot;&gt;An Overview of Stochastic Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://d2l.ai/chapter_optimization/index.html&quot;&gt;11. Optimization Algorithms â€” Dive into Deep Learning 0.16.6 documentation (d2l.ai)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://distill.pub/2017/momentum/&quot;&gt;Why Momentum Really Works (distill.pub)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/245502/why-should-we-shuffle-data-while-training-a-neural-network&quot;&gt;Why should we shuffle data while training a neural network?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/&quot;&gt;A Gentle Introduction to Batch Normalization for Deep Neural Networks (machinelearningmastery.com)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/&quot;&gt;A Gentle Introduction to Early Stopping to Avoid Overtraining Neural Networks (machinelearningmastery.com)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;å‚è€ƒæ–‡çŒ®&quot;&gt;å‚è€ƒæ–‡çŒ®&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;H. Robinds and S. Monro, â€œA stochastic approximation method,â€ Annals of Mathematical Statistics, vol. 22, pp. 400â€“407, 1951. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref1&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Darken, C., Chang, J., &amp;amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1â€“11. &lt;a href=&quot;https://hub.pubmedplus.com/10.1109/NNSP.1992.253713&quot;&gt;http://doi.org/10.1109/NNSP.1992.253713&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref2&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp;amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1â€“14. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1406.2572&quot;&gt;http://arxiv.org/abs/1406.2572&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref3&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref4&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145â€“151. &lt;a href=&quot;https://hub.pubmedplus.com/10.1016/S0893-6080(98)00116-6&quot;&gt;http://doi.org/10.1016/S0893-6080(98)00116-6&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref5&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543â€“ 547. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref6&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bengio, Y., Boulanger-Lewandowski, N., &amp;amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1212.0901&quot;&gt;http://arxiv.org/abs/1212.0901&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref7&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref8&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Duchi, J., Hazan, E., &amp;amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121â€“2159. Retrieved from &lt;a href=&quot;http://jmlr.org/papers/v12/duchi11a.html&quot;&gt;http://jmlr.org/papers/v12/duchi11a.html&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref9&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, â€¦ Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1â€“11. &lt;a href=&quot;http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf&quot;&gt;http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref10&quot;&gt;â†©ï¸&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref10:1&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532â€“1543. &lt;a href=&quot;https://hub.pubmedplus.com/10.3115/v1/D14-1162&quot;&gt;http://doi.org/10.3115/v1/D14-1162&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref11&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Duchi et al. [3] give this matrix as an alternative to the &lt;em&gt;full&lt;/em&gt; matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters dd. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref12&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1212.5701&quot;&gt;http://arxiv.org/abs/1212.5701&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref13&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Kingma, D. P., &amp;amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1â€“13. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref14&quot;&gt;â†©ï¸&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref14:1&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., &amp;amp; Hochreiter, S. (2017). GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems 30 (NIPS 2017). &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref15&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dozat, T. (2016). Incorporating Nesterov Momentum into Adam. ICLR Workshop, (1), 2013â€“2016. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref16&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Huang, G., Liu, Z., Weinberger, K. Q., &amp;amp; van der Maaten, L. (2017). Densely Connected Convolutional Networks. In Proceedings of CVPR 2017. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref17&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Johnson, M., Schuster, M., Le, Q. V, Krikun, M., Wu, Y., Chen, Z., â€¦ Dean, J. (2016). Googleâ€™s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. arXiv Preprint arXiv:1611.0455. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref18&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Reddi, Sashank J., Kale, Satyen, &amp;amp; Kumar, Sanjiv. On the Convergence of Adam and Beyond. Proceedings of ICLR 2018. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref19&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Loshchilov, I., &amp;amp; Hutter, F. (2019). Decoupled Weight Decay Regularization. In Proceedings of ICLR 2019. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref20&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ma, J., &amp;amp; Yarats, D. (2019). Quasi-hyperbolic momentum and Adam for deep learning. In Proceedings of ICLR 2019. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref21&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lucas, J., Sun, S., Zemel, R., &amp;amp; Grosse, R. (2019). Aggregated Momentum: Stability Through Passive Damping. In Proceedings of ICLR 2019. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref22&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Niu, F., Recht, B., Christopher, R., &amp;amp; Wright, S. J. (2011). Hogwild! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1â€“22. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref23&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Mcmahan, H. B., &amp;amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1â€“9. Retrieved from &lt;a href=&quot;http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf&quot;&gt;http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref24&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., â€¦ Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref25&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zhang, S., Choromanska, A., &amp;amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1â€“24. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1412.6651&quot;&gt;http://arxiv.org/abs/1412.6651&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref26&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LeCun, Y., Bottou, L., Orr, G. B., &amp;amp; MÃ¼ller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9â€“50. &lt;a href=&quot;https://hub.pubmedplus.com/10.1007/3-540-49430-8_2&quot;&gt;http://doi.org/10.1007/3-540-49430-8_2&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref27&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bengio, Y., Louradour, J., Collobert, R., &amp;amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41â€“48. &lt;a href=&quot;https://hub.pubmedplus.com/10.1145/1553374.1553380&quot;&gt;http://doi.org/10.1145/1553374.1553380&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref28&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zaremba, W., &amp;amp; Sutskever, I. (2014). Learning to Execute, 1â€“25. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1410.4615&quot;&gt;http://arxiv.org/abs/1410.4615&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref29&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ioffe, S., &amp;amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3. &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref30&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp;amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1â€“11. Retrieved from &lt;a href=&quot;https://arxiv.org/abs/1511.06807&quot;&gt;http://arxiv.org/abs/1511.06807&lt;/a&gt; &lt;a href=&quot;https://ruder.io/optimizing-gradient-descent/index.html#fnref31&quot;&gt;â†©ï¸&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Tianxin Xie</name></author><category term="MachineLearning" /><category term="MachineLearning" /><summary type="html">åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ€§èƒ½ã€‚æœ¬æ–‡ä¸»è¦å‚è€ƒä¸€ç¯‡ reviewï¼Œæ€»ç»“äº†åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å¸¸ç”¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç¿»è¯‘è‡ªè¯¥ç¯‡ reviewï¼Œåšäº†å¢åˆ æ”¹ï¼Œå¹¶åŠ ä¸Šäº†éƒ¨åˆ†è‡ªå·±çš„ç†è§£ï¼Œä»¥åŠé¢å¤–çš„å›¾ç¤ºã€è§†é¢‘å’Œé˜…è¯»ææ–™ã€‚</summary></entry><entry><title type="html">æœºå™¨å­¦ä¹ ç¬”è®° - æ¿€æ´»å‡½æ•°æ€»ç»“</title><link href="https://xietx1995.github.io/2021/activations" rel="alternate" type="text/html" title="æœºå™¨å­¦ä¹ ç¬”è®° - æ¿€æ´»å‡½æ•°æ€»ç»“" /><published>2021-06-30T00:00:00+08:00</published><updated>2021-06-30T00:00:00+08:00</updated><id>https://xietx1995.github.io/2021/MLNotes_Activations</id><content type="html" xml:base="https://xietx1995.github.io/2021/activations">&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/neuron.png&quot; alt=&quot;neuron&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;æ¿€æ´»å‡½æ•°æ¨¡æ‹Ÿäº†å¤§è„‘ä¸­ç¥ç»å…ƒçš„ä¿¡å·ä¼ é€’ï¼Œä¸Šå›¾æ˜¯ç¥ç»å…ƒå›¾ç¤ºï¼Œä¸‹å›¾æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„æ•°å­¦æ¨¡å‹ï¼ˆå›¾ç‰‡å‡æ¥è‡ª&lt;a href=&quot;[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-1/#quick)&quot;&gt;CS231n&lt;/a&gt;ï¼‰ã€‚æœ¬æ–‡æ€»ç»“äº†å¤§éƒ¨åˆ†æ¿€æ´»å‡½æ•°ï¼ŒåŒ…æ‹¬ä»–ä»¬æ•°å­¦è¡¨ç¤ºã€æ€§è´¨ã€ä¼˜ç¼ºç‚¹ã€ä»¥åŠå¯¼æ•°è®¡ç®—ã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/neuron_model.jpeg&quot; alt=&quot;neural_model&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sigmoid&quot;&gt;Sigmoid&lt;/h2&gt;

&lt;p&gt;æ›¾ç»åœ¨ç½‘ç»œä¸­ç”¨å¾—å¾ˆå¤šï¼Œç°åœ¨ä¸æ¨èåœ¨ç½‘ç»œçš„ä¸­é—´å±‚ä½¿ç”¨ï¼Œå¯ä»¥ä½œä¸ºè¾“å‡ºå±‚ä½¿ç”¨ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[\delta(x) = \frac{1}{1+e^{-x}}\]

&lt;p&gt;&lt;strong&gt;å¯¼æ•°&lt;/strong&gt;ï¼š&lt;/p&gt;

\[\delta&apos;(x) = \delta(x)(1-\delta(x))\]

&lt;p&gt;&lt;strong&gt;å›¾åƒ&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/sigmoid.png&quot; alt=&quot;relu&quot; style=&quot;zoom:25%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ€§è´¨/ä½œç”¨&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;å°†è¾“å…¥è§„æ•´åˆ° $[0, 1]$ çš„åŒºé—´ï¼›&lt;/li&gt;
  &lt;li&gt;æ­£å¥½èƒ½â€œè§£é‡Šâ€ç¬¦åˆç¥ç»å…ƒçš„â€œæ¿€å‘â€ï¼ˆsigmoid æ›¾å› æ­¤å¾ˆå—æ¬¢è¿ï¼‰ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ä»å›¾åƒå¯ä»¥çœ‹å‡ºï¼Œè¿œç¦»åæ ‡åŸç‚¹çš„è¾“å…¥ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼›&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sigmoid çš„è¾“å‡ºä¸æ˜¯ä»¥ 0 ä¸ºä¸­å¿ƒçš„ï¼Œè€Œæ˜¯å…¨ä¸ºéè´Ÿæ•°ï¼Œä¼šå¯¼è‡´å‡è®¾ç©ºé—´è¢«ç¼©å°ï¼ˆç»´åº¦è¶Šé«˜è¶Šä¸¥é‡ï¼‰ï¼Œå¾ˆå¯èƒ½å­¦ä¸åˆ°æœ€ä¼˜å‚æ•°ï¼›&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/gradient_directions.png&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;åœ¨å®é™…åº”ç”¨æ—¶æŒ‡æ•°è¿ç®—é€Ÿåº¦æ…¢ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;å»ºè®®&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ä¸æ¨èåœ¨ç½‘ç»œä¸­é—´å±‚ä½¿ç”¨ï¼›&lt;/li&gt;
  &lt;li&gt;å¯ä½œä¸ºè¾“å‡ºå±‚ä½¿ç”¨ï¼ˆå¦‚logistics regressionï¼‰ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tanh&quot;&gt;Tanh&lt;/h2&gt;

&lt;p&gt;tanh å‡½æ•°å’Œ sigmoid æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ï¼Œä½†æ˜¯æ¯” sigmoid å¥½ä¸€ç‚¹çš„æ˜¯ tanh çš„è¾“å‡ºæ˜¯ä»¥ 0 ä¸ºä¸­å¿ƒçš„ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \frac{e^{2x}-1}{e^{2x}+1}\]

&lt;p&gt;&lt;strong&gt;å¯¼æ•°&lt;/strong&gt;ï¼š&lt;/p&gt;

\[tanh&apos;(x) = 1 - tanh^2(x)\]

&lt;p&gt;&lt;strong&gt;å›¾åƒ&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/tanh.png&quot; alt=&quot;relu&quot; style=&quot;zoom:25%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ€§è´¨/ä½œç”¨&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;å°†è¾“å…¥è§„æ•´åˆ° $[-1, 1]$ çš„åŒºé—´ï¼›&lt;/li&gt;
  &lt;li&gt;ä»¥ 0 ä¸ºä¸­å¿ƒï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ä»å›¾åƒå¯ä»¥çœ‹å‡ºï¼Œè¿œç¦»åæ ‡åŸç‚¹çš„è¾“å…¥ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼›&lt;/li&gt;
  &lt;li&gt;åœ¨å®é™…åº”ç”¨æ—¶æŒ‡æ•°è¿ç®—é€Ÿåº¦æ…¢ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;å»ºè®®&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ä¸æ¨èåœ¨ç½‘ç»œä¸­é—´å±‚ä½¿ç”¨ï¼›&lt;/li&gt;
  &lt;li&gt;å¯ä½œä¸ºè¾“å‡ºå±‚ä½¿ç”¨ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relu&quot;&gt;ReLU&lt;/h2&gt;

&lt;p&gt;ReLU çš„å…¨ç§°æ˜¯ Rectified Linear Unitï¼ˆçº¿æ€§ä¿®æ­£å•å…ƒï¼‰ï¼Œç›¸è¾ƒäº sigmoid å’Œ tanh æœ‰è¯¸å¤šå¥½å¤„ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[f(x) = max(0, x)\]

&lt;p&gt;&lt;strong&gt;å¯¼æ•°&lt;/strong&gt;ï¼š&lt;/p&gt;

\[f&apos;(x) =
\begin{cases}
0, &amp;amp; \text{if $x \le 0$} \\
1, &amp;amp; \text{if $x &amp;gt; 0$}
\end{cases}\]

&lt;p&gt;&lt;strong&gt;å›¾åƒ&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/relu.png&quot; alt=&quot;relu&quot; style=&quot;zoom:25%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ€§è´¨/ä½œç”¨&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;è¾“å…¥å¤§äº 0 æ—¶ï¼Œä¸ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ï¼›&lt;/li&gt;
  &lt;li&gt;è®¡ç®—æ•ˆç‡é«˜ï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ï¼›&lt;/li&gt;
  &lt;li&gt;æ”¶æ•›é€Ÿåº¦æ¯” sigmoid / tanh å¿«å¾—å¤šï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ä¸æ˜¯ä»¥ 0 ä¸ºä¸­å¿ƒçš„ï¼Œå’Œ sigmoid æœ‰åŒæ ·çš„é—®é¢˜ï¼›&lt;/li&gt;
  &lt;li&gt;åœ¨è¾“å…¥å°äº 0 æ—¶ä¼šæœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;å»ºè®®&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;æ¨èä½¿ç”¨ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;leaky-relu&quot;&gt;Leaky ReLU&lt;/h2&gt;

&lt;p&gt;Leaky ReLU æ˜¯ ReLU çš„æ”¹è¿›ï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³è¾“å…¥å°äº 0 æ—¶çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[f(x) = max(\alpha x, x)\]

&lt;p&gt;$\alpha$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œé€šå¸¸å– 0.1ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å¯¼æ•°&lt;/strong&gt;ï¼š&lt;/p&gt;

\[f&apos;(x) =
\begin{cases}
\alpha, &amp;amp; \text{if $x \le 0$} \\
1, &amp;amp; \text{if $x &amp;gt; 0$}
\end{cases}\]

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/leaky_relu.png&quot; alt=&quot;relu&quot; style=&quot;zoom:30%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ€§è´¨/ä½œç”¨&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ä¸ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ï¼›&lt;/li&gt;
  &lt;li&gt;è®¡ç®—æ•ˆç‡é«˜ï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ï¼›&lt;/li&gt;
  &lt;li&gt;æ”¶æ•›é€Ÿåº¦æ¯” sigmoid / tanh å¿«å¾—å¤šï¼ˆ&lt;font color=&quot;green&quot;&gt;Good&lt;/font&gt;ï¼‰ï¼›&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;å˜ç§&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;å°† $\alpha$ è®¾ç½®ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼ˆè€Œä¸æ˜¯æ‰‹åŠ¨è®¾å®šï¼‰ï¼Œç§°ä¸ºPReLUã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;å»ºè®®&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;æ¨èä½¿ç”¨ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;å…¶ä»–æ¿€æ´»å‡½æ•°&quot;&gt;å…¶ä»–æ¿€æ´»å‡½æ•°&lt;/h2&gt;

&lt;p&gt;è¿™é‡Œä¸»è¦åˆ—å‡ºäº†å¦å¤– 3 ä¸ªä¸å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exponential Linear Unit (ELU)&lt;/li&gt;
  &lt;li&gt;Scaled Exponential Linear Unit (SELU)&lt;/li&gt;
  &lt;li&gt;Gaussian Error Linear Unit (GELU)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elu&quot;&gt;ELU&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[f(x) =
\begin{cases}
x, &amp;amp; \text{if $x &amp;gt; 0$} \\
\alpha(e^x-1), &amp;amp; \text{if $x \le 0$}
\end{cases}\]

&lt;p&gt;$ \alpha $ æ˜¯è¶…å‚æ•°ï¼Œé»˜è®¤å– 1ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å›¾åƒ&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/elu.png&quot; alt=&quot;elu&quot; style=&quot;zoom:25%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ€§è´¨/ä½œç”¨&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;å…·æœ‰ ReLU çš„æ‰€æœ‰ä¼˜ç‚¹ï¼›&lt;/li&gt;
  &lt;li&gt;å¹³å‡å€¼æ›´åŠ é è¿‘ 0ï¼›&lt;/li&gt;
  &lt;li&gt;è¾“å…¥å€¼ä¸ºè´Ÿæ•°æ—¶è¾ƒ Leaky ReLU æ›´åŠ é²æ£’ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;è®¡ç®—é‡å¤§&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;selu&quot;&gt;SELU&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[selu(x) =
\begin{cases}
\lambda x, &amp;amp; \text{if $x &amp;gt; 0$} \\
\lambda\alpha(e^x-1), &amp;amp; \text{if $x \le 0$}
\end{cases}\]

&lt;p&gt;è®ºæ–‡â€Klambauer et al, Self-Normalizing Neural Networks, ICLR 2017â€œç»™å‡ºäº†æœ€ä½³å–å€¼ï¼š&lt;/p&gt;

&lt;p&gt;$\alpha = 1.6732632423543772848170429916717$&lt;/p&gt;

&lt;p&gt;$\lambda = 1.0507009873554804934193349852946$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å›¾åƒ&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/selu.png&quot; alt=&quot;selu&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ€§è´¨/ä½œç”¨&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ELU çš„ Scaled ç‰ˆæœ¬ï¼Œå¯¹äºå¾ˆæ·±çš„ç½‘ç»œæ•ˆæœè¾ƒå¥½ï¼›&lt;/li&gt;
  &lt;li&gt;å…·æœ‰â€Self-Normalizingâ€œæ•ˆæœï¼Œè®­ç»ƒå¾ˆæ·±çš„ç½‘ç»œæ—¶æ— éœ€ batch normalizationã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ç¼ºç‚¹&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;è®¡ç®—é‡å¤§&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gelu&quot;&gt;GELU&lt;/h3&gt;

&lt;p&gt;GELU è¾ƒä¸ºå¤æ‚ï¼Œä¸»è¦æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼ˆCopy è‡ª EECS 498-007 Slidesï¼‰ï¼š&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Idea: Multiply input by 0 or 1  at random; large values more  likely to be multiplied by 1,  small values more likely to be  multiplied by 0 (data-dependent dropout)&lt;/li&gt;
  &lt;li&gt;Take expectation over  randomness&lt;/li&gt;
  &lt;li&gt;Very common in Transformers (BERT, GPT, GPT-2, GPT-3)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;å…¬å¼&lt;/strong&gt;ï¼š&lt;/p&gt;

\[\begin{align}
gelu(x) &amp;amp;= xP(X \le x) \\
&amp;amp;= \frac{x}{2}(1+erf(x/\sqrt{2})) \\
&amp;amp;\approx x\delta(1.702x)
\end{align}\]

&lt;p&gt;å…¶ä¸­éšæœºå˜é‡ $X \thicksim N(0,1)$ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å›¾åƒ&lt;/strong&gt;ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/gelu.png&quot; alt=&quot;gelu&quot; style=&quot;zoom:25%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;å…·ä½“ç»†èŠ‚å¯ä»¥é˜…è¯»è®ºæ–‡ï¼š&lt;a href=&quot;https://arxiv.org/pdf/1606.08415v3.pdf&quot;&gt;https://arxiv.org/pdf/1606.08415v3.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;å„ç§æ¿€æ´»å‡½æ•°å¯¹æ¯”&quot;&gt;å„ç§æ¿€æ´»å‡½æ•°å¯¹æ¯”&lt;/h2&gt;

&lt;p&gt;ä¸‹é¢æ˜¯åœ¨ CIFAR-10 ä¸Šå„ç§æ¿€æ´»å‡½æ•°çš„è¡¨ç°ï¼ˆå›¾åƒæ¥è‡ªè®ºæ–‡ &lt;a href=&quot;https://arxiv.org/abs/1710.05941&quot;&gt;https://arxiv.org/abs/1710.05941&lt;/a&gt;ï¼‰ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Activations.assets/accuracy_of_activations.png&quot; alt=&quot;activations performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;æ€»çš„æ¥è¯´ï¼Œä¸æƒ³æŠ˜è…¾çš„è¯ï¼Œç”¨ ReLU å°±è¡Œäº†ï¼Œåˆ«ç”¨ sigmoid å’Œ tanhã€‚&lt;/p&gt;

&lt;h2 id=&quot;å‚è€ƒæ–‡ç« &quot;&gt;å‚è€ƒæ–‡ç« &lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/neural-networks-1/&quot;&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.eecs.umich.edu/~justincj/slides/eecs498/FA2020/598_FA2020_lecture10.pdf&quot;&gt;598_FA2020_lecture10.pdf (umich.edu)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/&quot;&gt;A Gentle Introduction to the Rectified Linear Unit (ReLU)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.math24.net/derivatives-hyperbolic-functions&quot;&gt;Derivatives of Hyperbolic Functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c&quot;&gt;GELU activation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GELU paper- &lt;a href=&quot;https://arxiv.org/pdf/1606.08415v3.pdf&quot;&gt;https://arxiv.org/pdf/1606.08415v3.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.05941&quot;&gt;https://arxiv.org/abs/1710.05941&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Tianxin Xie</name></author><category term="MachineLearning" /><category term="MachineLearning" /><summary type="html"></summary></entry><entry><title type="html">æœºå™¨å­¦ä¹ ç¬”è®° - Softmax</title><link href="https://xietx1995.github.io/2021/softmax" rel="alternate" type="text/html" title="æœºå™¨å­¦ä¹ ç¬”è®° - Softmax" /><published>2021-06-17T00:00:00+08:00</published><updated>2021-06-17T00:00:00+08:00</updated><id>https://xietx1995.github.io/2021/MLNotes_Softmax</id><content type="html" xml:base="https://xietx1995.github.io/2021/softmax">&lt;p&gt;Softmax å¸¸ç”¨äºå¤šåˆ†ç±»çš„æƒ…å†µï¼Œå®ƒå°†ä¸€ä¸ª N ç»´çš„ç”±ä»»æ„å®æ•°ç»„æˆçš„å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç„¶åè¾“å‡ºä¸€ä¸ª N ç»´çš„æ¦‚ç‡å‘é‡ï¼Œä¸”å…¶æ‰€æœ‰åˆ†é‡ä¹‹å’Œä¸º 1ã€‚æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸ªæœ‰ 5 ç±»ï¼ˆç±»åˆ«ç¼–å·ä¸º 0, 1, 2, 3, 4ï¼‰è¾“å‡ºçš„å¤šåˆ†ç±»å™¨ï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_imgs/MLNotes_Softmax.assets/softmax01.png&quot; alt=&quot;softmax01&quot; title=&quot;5ä¸ªç±»åˆ«çš„Softmaxåˆ†ç±»å™¨&quot; /&gt;&lt;/p&gt;

&lt;p&gt;å¦‚å›¾æ‰€ç¤ºï¼Œ$X_i$ è¡¨ç¤ºæ•°æ®é›†ä¸­ç¬¬ $i$ ä¸ªæ ·æœ¬ï¼Œæœ‰ N ä¸ªç‰¹å¾ï¼›$W$ è¡¨ç¤ºé€šè¿‡å­¦ä¹ å¾—åˆ°çš„å‚æ•°çŸ©é˜µï¼ˆN è¡Œ 5 åˆ—ï¼‰ï¼›$S_i$ è¡¨ç¤ºç»è¿‡è®¡ç®—åï¼Œè¾“å…¥æ ·æœ¬åœ¨å„ä¸ªç±»åˆ«ä¸Šçš„å¾—åˆ†ã€‚æ¥ç€å°†å¾—åˆ†ç»è¿‡â€Softmaxâ€å¤„ç†åï¼Œå¾—åˆ°æ ·æœ¬å±äºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡å‘é‡ $P_i$ï¼Œæœ€åé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ä½œä¸ºè¾“å‡ºæ ‡ç­¾ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼ŒSoftmax åˆ†ç±»å™¨æ”¾ç½®åœ¨æ•´ä¸ªæ¨¡å‹çš„ç»“å°¾å¤„ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸Šå›¾ä¸­ Softmax çš„è¾“å…¥ $X_i$ ä¹Ÿå¯ä»¥æ˜¯å‰ç½®æ¨¡å—ï¼ˆä¾‹å¦‚ç¥ç»ç½‘ç»œ)çš„è¾“å‡ºã€‚&lt;/p&gt;

&lt;h2 id=&quot;1-é¢„æµ‹æµç¨‹&quot;&gt;1 é¢„æµ‹æµç¨‹&lt;/h2&gt;

&lt;p&gt;ä¸Šå›¾ä¸­ç»¿è‰²éƒ¨åˆ†è¡¨ç¤ºçš„å¾—åˆ†æ˜¯è¾“å…¥ $X_i$ å’Œ $W$ çš„çº¿æ€§ä¹˜ç§¯ï¼š&lt;/p&gt;

\[S_i = f(X_i, W) = X_i * W\]

&lt;p&gt;æ¥ç€ï¼Œç”¨ $S_i$ çš„æ¯ä¸ªåˆ†é‡ $S_{ik}$ ä½œä¸ºæŒ‡æ•°ï¼Œåšå¦‚ä¸‹è®¡ç®—å°±å¾—åˆ°äº†æ¦‚ç‡å‘é‡ $P_i$ï¼š&lt;/p&gt;

\[P_i = [P_{i,0} = \frac{e^{S_{i,0}}}{\sum_{k=0}^{4}e^{S_{i,k}}}, P_{i,1} = \frac{e^{S_{i,1}}}{\sum_{k=0}^{4}e^{S_{i,k}}}, ..., P_{i,4} = \frac{e^{S_{i,4}}}{\sum_{k=0}^{4}e^{S_{i,k}}}]\]

&lt;p&gt;ä¸‹é¢ç”¨ numpy ç®€å•æ¨¡æ‹Ÿä¸€ä¸‹è¿™ä¸ªè®¡ç®—è¿‡ç¨‹ï¼Œé¦–å…ˆç”Ÿæˆä¸€ä¸ªéšæœºçš„ $X_i$ å’Œ $W$ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.79133497&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.25991782&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.05647627&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.57785825&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.09202159&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.40157209&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.19652735&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.04938573&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.10461471&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.74042572&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.65392227&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.89659498&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.11629679&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.93221547&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.77651529&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.07090883&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.15001729&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1281176&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.35069474&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.61484098&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.01960799&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.97765036&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.95673941&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1911482&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.51133051&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.52650448&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.01087258&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.22877655&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.67891746&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.64167637&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.22563436&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.17559678&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.41271418&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.7964071&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0986726&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.20037395&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.68407372&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.25964979&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.73397112&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.09695987&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.02523734&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.70474257&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.76772239&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.19074719&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.19775151&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.28158624&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.21311108&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.23231037&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.38678946&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.91647861&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.15435732&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.67917135&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.07906748&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.81204621&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.42509496&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.46912629&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.54266569&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0158846&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.53389348&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7792223&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;è®¡ç®—è¯¥æ ·æœ¬åœ¨å„ç±»åˆ«ä¸Šçš„å¾—åˆ†ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# è®¡ç®—æ ·æœ¬åœ¨å„ç±»åˆ«çš„å¾—åˆ†
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.13973271&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.64507675&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.55969798&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.8374945&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.38799113&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;æ±‚æ¦‚ç‡å‘é‡ $P_i$ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ä½¿ç”¨å¾—åˆ†sæ±‚æŒ‡æ•°
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01592711&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.19299775&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.21019955&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05857224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.03377646&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# æ±‚å„ä¸ªåˆ†ç±»æ¦‚ç‡
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.03113968&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.37733705&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.41096892&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.11451675&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.06603761&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# æ¦‚ç‡ä¹‹å’Œåº”è¯¥ä¸º1
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ä½¿ç”¨ &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.argmax.html&quot;&gt;numpy.argmax()&lt;/a&gt; å‡½æ•°å¯ä»¥è·å¾—æœ€å¤§å€¼çš„ä¸‹æ ‡ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ä¸‹æ ‡ä»0å¼€å§‹
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;è·å¾—çš„ä¸‹æ ‡å³æ˜¯ Softmax ç»™å‡ºçš„å…³äºè¾“å…¥ $X_i$ çš„ç±»åˆ«æ ‡ç­¾ã€‚&lt;/p&gt;

&lt;h2 id=&quot;2-loss-å‡½æ•°&quot;&gt;2 Loss å‡½æ•°&lt;/h2&gt;

&lt;p&gt;Loss å‡½æ•°è¡¡é‡äº†æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ŒLoss è¶Šä½ï¼Œåœ¨è®­ç»ƒé›†ä¸Šçš„æ•ˆæœè¶Šå¥½ã€‚åœ¨ Softmax ä¸­ï¼Œæ ·æœ¬çš„æ ‡ç­¾é€šå¸¸ç”¨ one-hot è¡¨ç¤ºï¼Œè‹¥è®­ç»ƒæ ·æœ¬ $X_i$ çš„ç±»åˆ«ç¼–å·ä¸º 2ï¼Œåˆ™å…¶æ ‡ç­¾ $y_i = [0, 0, 1, 0, 0]$ ã€‚Softmax ä½¿ç”¨&lt;a href=&quot;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&quot;&gt;äº¤å‰ç†µ&lt;/a&gt;ä½œä¸º Loss æŸå¤±ï¼Œå¯¹æ ·æœ¬ $X_i$ï¼Œå…¶æŸå¤± $L_i$ å¦‚ä¸‹ï¼š&lt;/p&gt;

\[L_i = -log(\frac{e^{S_{i,2}}}{\sum_{k=0}^{4}{e^{S_{i,k}}}})\]

&lt;p&gt;ä¸Šå¼ä¸­çš„ $S_{i2}$ è¡¨ç¤ºè®­ç»ƒæ ·æœ¬åœ¨ç¬¬ 2 ç±»ä¸Šå¯¹åº”çš„å¾—åˆ†ã€‚å¯ä»¥æŒ‰ç…§ 3.1 èŠ‚ä¸­çš„å½¢å¼è¡¨ç¤ºä¸ºï¼š&lt;/p&gt;

\[L_i = -log(P_{i,2})\]

&lt;p&gt;æˆ‘ä»¬å¯ä»¥ç»§ç»­ç”¨ 3.1 èŠ‚ä¸­çš„ä¾‹å­æ¥è¯´æ˜ï¼š&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.03113968&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.37733705&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.41096892&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.11451675&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.06603761&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# æ ·æœ¬æ ‡ç­¾
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Li&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# è®¡ç®—Liï¼Œæ³¨æ„p*yiæ˜¯å…ƒç´ å¯¹åº”ç›¸ä¹˜
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Li&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.8892376972266337&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºä¸¤ä¸ªåˆ†å¸ƒ $p$ å’Œ $q$ï¼Œå®ƒä»¬çš„äº¤å‰ç†µçš„å®šä¹‰å¦‚ä¸‹ï¼š&lt;/p&gt;

\[xent(p,q)=-\sum_{k}{p(k)log(q(k))}\]

&lt;p&gt;Softmax è¾“å‡ºçš„æ¦‚ç‡æˆ‘ä»¬å¯ä»¥å½“åšä¸€ä¸ªåˆ†å¸ƒï¼Œä¾‹å¦‚æ ·æœ¬ $X_i$ å¯¹åº”çš„è¾“å‡º $p_i = [0.1, 0.3, 0.2, 0.15, 0.25]$ï¼Œå…¶æ ‡ç­¾ä¸º $y_i = [0, 0, 1, 0, 0]$ï¼Œé‚£ä¹ˆå®ƒä»¬çš„äº¤å‰ç†µä¸ºï¼š&lt;/p&gt;

\[\begin{align}
xent(p_i, y_i)
&amp;amp;= -(0*log(0.1)+0*log(0.3)+1*log(0.2)+0*log(0.15)+0*log(0.25)) \nonumber \\
&amp;amp;= -1*log(0.2) \nonumber \\
&amp;amp;= -log(0.2) \nonumber \\
&amp;amp;= -log(p_{i2}) \nonumber \\
&amp;amp;= 1.61 \nonumber
\end{align}\]

&lt;p&gt;å› ä¸ºæ ‡ç­¾ä¸­åªæœ‰ä¸€ä¸ª 1ï¼Œå…¶ä»–å…¨æ˜¯ 0ï¼Œæ‰€ä»¥å•ä¸ªæ ·æœ¬ Softmax çš„ Loss å¯ä»¥ç®€å†™æˆï¼š&lt;/p&gt;

\[L_i = -log(\frac{e^{S_{i,t}}}{\sum_{k}{e^{S_{i,k}}}})\]

&lt;p&gt;$S_{it}$ (t for target)è¡¨ç¤ºæ ·æœ¬ç±»åˆ«æ ‡ç­¾å¯¹åº”çš„å¾—åˆ†ã€‚&lt;/p&gt;

&lt;p&gt;ä¸Šä¾‹ä¸­ $S_{it}=0.2$ï¼Œ Loss ä¸º 1.61ã€‚è‹¥ $p_i = [0.1, 0.05, 0.7, 0.1, 0.05]$ï¼Œå³ $S_{it}=0.7$ï¼Œåˆ™ $L_i = -log(0.7) = 0.36$ã€‚å¯è§ï¼Œæ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡å’Œæ ·æœ¬ç±»åˆ«æ ‡ç­¾è¶Šæ¥è¿‘ï¼ŒLoss è¶Šå°ï¼Œè¯´æ˜æ¨¡å‹è¶Šå‡†ç¡®ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆä½¿ç”¨äº¤å‰ç†µä½œä¸º Loss çš„åŸå› ã€‚&lt;/p&gt;

&lt;p&gt;æ•´ä¸ªè®­ç»ƒé›†çš„ Loss ä¸ºå„ä¸ªæ ·æœ¬ Loss çš„å¹³å‡å€¼ï¼Œé€šå¸¸è¿˜ä¼šåŠ ä¸Š L2 æƒ©ç½šé¡¹ï¼Œç³»æ•° $\frac{1}{2}$ æ˜¯ä¸ºäº†æ±‚å¯¼æ¶ˆæ‰æŒ‡æ•°ï¼š&lt;/p&gt;

\[L = \frac{1}{N} \sum\_{i=1}^{N} L\_i + \frac{\lambda}{2}\||W\||^2\]

&lt;p&gt;æˆ‘ä»¬ä¸€èˆ¬ç§°å…¶ä¸º Softmax Lossã€‚&lt;/p&gt;

&lt;h2 id=&quot;3-æ¢¯åº¦è®¡ç®—&quot;&gt;3 æ¢¯åº¦è®¡ç®—&lt;/h2&gt;

&lt;p&gt;å…ˆå›é¡¾ä¸€ä¸‹ Softmax çš„æµç¨‹ï¼š&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;graph LR
    A[Xi * W] --&amp;gt; Si --&amp;gt; Softmaxè¿ç®— --&amp;gt; Pi --&amp;gt; B[Li=logPi]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ç¬¦å·è¯´æ˜ï¼š&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ç¬¦å·&lt;/th&gt;
      &lt;th&gt;å«ä¹‰&lt;/th&gt;
      &lt;th&gt;ç»´åº¦&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$X_i$&lt;/td&gt;
      &lt;td&gt;ä¸€ä¸ªè¾“å…¥æ ·æœ¬å‘é‡&lt;/td&gt;
      &lt;td&gt;1 x M&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W$&lt;/td&gt;
      &lt;td&gt;å‚æ•°çŸ©é˜µ&lt;/td&gt;
      &lt;td&gt;M x C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$S_i$&lt;/td&gt;
      &lt;td&gt;æ¯ä¸ªç±»åˆ«ä¸Šçš„å¾—åˆ†ç»„æˆçš„å‘é‡&lt;/td&gt;
      &lt;td&gt;1 x C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$P_i$&lt;/td&gt;
      &lt;td&gt;æ¯ä¸ªç±»åˆ«ä¸Šçš„æ¦‚ç‡ç»„æˆçš„å‘é‡&lt;/td&gt;
      &lt;td&gt;1 x C&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_i$&lt;/td&gt;
      &lt;td&gt;æ ·æœ¬æ ‡ç­¾ one-hot å‘é‡&lt;/td&gt;
      &lt;td&gt;1 x C&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;å…¶ä¸­ $M$ æ˜¯è¾“å…¥çš„ç‰¹å¾æ•°é‡ï¼Œ$C$ æ˜¯è¾“å‡ºç±»åˆ«çš„æ•°é‡ã€‚æˆ‘ä»¬çš„ç›®çš„ï¼Œæ˜¯è¦æ±‚ Loss å‡½æ•°å…³äºå‚æ•°çŸ©é˜µ $W$ çš„åå¯¼æ•°ï¼Œè¿™ä¸ªåå¯¼æ•°å°†ç”¨äºåœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­æ›´æ–° $W$ çš„å€¼ã€‚æ ¹æ®é“¾å¼æ³•åˆ™æœ‰ï¼š&lt;/p&gt;

\[\frac{\partial{L_i}}{\partial{W}}
= \frac{\partial{L_i}}{\partial{Si}} \times \frac{\partial{S_i}}{\partial{W}}\]

&lt;p&gt;ç¬¬ä¸€é¡¹å³ Softmax çš„åå¯¼æ•°ï¼Œç¬¬äºŒé¡¹åˆ™æ˜¯ä¸€ä¸ªçŸ©é˜µæ±‚å¯¼ï¼Œä¸‹é¢åˆ†åˆ«ä»‹ç»ã€‚&lt;/p&gt;

&lt;h3 id=&quot;31-softmax-loss-å¯¼æ•°&quot;&gt;3.1 Softmax Loss å¯¼æ•°&lt;/h3&gt;

&lt;p&gt;é¦–å…ˆæ¥è®¡ç®—æ ·æœ¬ $X_i$ çš„ Loss å…³äº $S_i$ çš„æ¢¯åº¦ $\frac{\partial{L_i}}{\partial{S_i}}$ï¼Œè¿™æ˜¯æ±‚æ ‡é‡å…³äºå‘é‡çš„åå¯¼æ•°ã€‚æˆ‘ä»¬çŸ¥é“ï¼š&lt;/p&gt;

\[L_i = -log(\frac{e^{S_{i,t}}}{\sum_{k}{e^{S_{i,k}}}})\]

&lt;p&gt;å…¶ä¸­ç±»åˆ« $t$ æ˜¯è¯¥æ ·æœ¬çš„æ ‡ç­¾ã€‚æŠŠä¸Šå¼ä¸­çš„åˆ†å¼æ”¹å†™ä¸ºå‡å¼ï¼š&lt;/p&gt;

\[L_i = -\left[log(e^{S_{i,t}}ï¼‰ - log({\sum_{k}{e^{S_{i,k}}}})\right]\]

&lt;p&gt;åŒ–ç®€å¹¶å±•å¼€æ±‚å’Œç¬¦å·ï¼š&lt;/p&gt;

\[L_i = -S_{it} + log(e^{S_{i,0}} + e^{S_{i,0}} + ... + e^{S_{i,C-1}})\]

&lt;p&gt;$log$ å‡½æ•°æ±‚å¯¼å¾ˆç®€å•ï¼Œå¦‚å¯¹åˆ†é‡ $e^{S_{ij}}$ æ±‚åå¯¼ï¼š&lt;/p&gt;

\[\begin{align}
\frac{\partial{log({\sum_{k}{e^{S_{i,k}}}})}}{\partial{e^{S_{i,j}}}}
&amp;amp;= \frac{e^{S_{i,j}}}{e^{S_{i,0}} + e^{S_{i,0}} + ... + e^{S_{i,C-1}}} \\
&amp;amp;= \frac{e^{S_{i,j}}}{\sum_{k}{e^{S_{i,k}}}} \\
&amp;amp;= P_{i,j}
\end{align}\]

&lt;p&gt;æ‰€ä»¥åé¢é‚£ä¸€é¡¹æ±‚å¯¼å…¶å®ç­‰äº Softmax è¾“å‡ºçš„æ¦‚ç‡å€¼ï¼Œé‚£ä¹ˆæ ‡é‡ $L_i$ å¯¹æŸä¸ªåˆ†é‡(æ ‡é‡) $S_{i,k}$ çš„åå¯¼æ•°ä¸ºï¼š&lt;/p&gt;

\[\frac{\partial{L_i}}{\partial{S_{i,k}}} =
\begin{cases}
    -1 + P_{i,k}, &amp;amp;\text{if $k = t$} \\
    P_{i,k}, &amp;amp;\text{if $k \neq t$}
\end{cases}\]

&lt;p&gt;å³è¯¥æ ·æœ¬çš„ Loss å¯¹æ ‡ç­¾å¯¹åº”çš„åˆ†é‡æ±‚åå¯¼æ—¶ä¼šå¤šå‡å»ä¸€ä¸ª $-1$ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬è®¡ç®— Softmax çš„ Loss å…³äº $S_i$ çš„å¯¼æ•°çš„æ—¶å€™ï¼Œç›´æ¥ä½¿ç”¨æ¦‚ç‡å‘é‡ $P_i$ å‡å»æ ‡ç­¾å‘é‡ $y_i$ å³å¯ï¼š&lt;/p&gt;

\[\frac{\partial{L_i}}{\partial{S_i}} = P_i - y_i\]

&lt;p&gt;å› æ­¤ï¼Œå¯¹äºæœ‰ N ä¸ªæ ·æœ¬çš„è®­ç»ƒé›† $X$ï¼Œå…¶è§„æ¨¡ä¸º N x Mï¼Œæ ‡ç­¾ç”¨ $y$ è¡¨ç¤ºï¼Œæ˜¯ä¸€ä¸ªç”± one-hot è¡Œå‘é‡ç»„æˆçš„ N x C çš„çŸ©é˜µã€‚æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§æ±‚å‡ºä¸€ä¸ª Loss å…³äº S çš„åå¯¼æ•°çŸ©é˜µï¼ˆè§„æ¨¡ä¸º N x C ï¼‰ï¼š&lt;/p&gt;

\[\begin{align}
    \frac{\partial{L}}{\partial{S}} &amp;amp;= P - y \\
    &amp;amp;=
    \begin{bmatrix}
    P_{0,0} &amp;amp; P_{0,1} &amp;amp; ... &amp;amp; P_{0,C-1} \\
    ... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\
    P_{N-1,0} &amp;amp; P_{N-1,1} &amp;amp; ... &amp;amp; P_{N-1,C-1}
    \end{bmatrix}
    -
    \begin{bmatrix}
    y_{0,0} &amp;amp; y_{0,1} &amp;amp; ... &amp;amp; y_{0,C-1} \\
    ... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\
    y_{N-1,0} &amp;amp; y_{N-1,1} &amp;amp; ... &amp;amp; y_{N-1,C-1}
    \end{bmatrix}
\end{align}\]

&lt;p&gt;åœ¨ç¼–å†™ä»£ç æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥æŒ‰ç…§ä¸Šé¢è¿™ä¸ªå¼å­æ‰¹é‡æ±‚å‡ºæ‰€æœ‰çš„ Softmax Loss åå¯¼æ•°ã€‚&lt;/p&gt;

&lt;h3 id=&quot;32-çŸ©é˜µå¯¼æ•°&quot;&gt;3.2 çŸ©é˜µå¯¼æ•°&lt;/h3&gt;

&lt;p&gt;æˆ‘ä»¬å·²ç»å®Œæˆäº†é“¾å¼æ±‚å¯¼çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œä¸‹é¢æ¥è®¡ç®—ç¬¬äºŒéƒ¨åˆ†ï¼Œå³ $\frac{\partial{S_i}}{\partial{W}}$ã€‚æˆ‘ä»¬å…ˆä»å•ä¸ªæ ·æœ¬çš„å¾—åˆ† $S_i$ å¼€å§‹ï¼Œåé¢å†æ¨å¯¼å‡ºå‘é‡åŒ–çš„è®¡ç®—å…¬å¼ã€‚æˆ‘ä»¬çŸ¥é“ $S_i = X_iW$ï¼Œå³ï¼š&lt;/p&gt;

\[[S_{i,0},S_{i,1},...,S_{i,C-1}] = [X_{i,0},X_{i,1},...,X_{i,M-1}]
\begin{bmatrix}
W_{0,0} &amp;amp; W_{0,1} &amp;amp; ... &amp;amp; W_{0,C-1} \\
W_{1,0} &amp;amp; W_{1,1} &amp;amp; ... &amp;amp; W_{1,C-1} \\
... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\
W_{M-1,0} &amp;amp; W_{M-1,1} &amp;amp; ... &amp;amp; W_{M-1,C-1} \\
\end{bmatrix}\]

&lt;p&gt;å¯è§ $\frac{\partial{S_i}}{\partial{W}}$ æ˜¯å‘é‡å¯¹çŸ©é˜µçš„åå¯¼æ•°ï¼Œç›´æ¥å†™å‡ºç­”æ¡ˆå¯èƒ½æ¯”è¾ƒå›°éš¾ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆè®¡ç®—æœ€ç®€å•çš„æ ‡é‡å¯¹æ ‡é‡çš„åå¯¼æ•°ï¼Œæœ€åæŠŠå®ƒä»¬ç»„åˆèµ·æ¥ã€‚ä¾‹å¦‚æ±‚ $S_{i,0}$ å…³äº $W_{0,0}$ çš„åå¯¼æ•°ï¼Œæˆ‘ä»¬çŸ¥é“ï¼š&lt;/p&gt;

\[S_{i,0} = X_{i,0}W_{0,0} + X_{i,1}W_{1,0} + ... + X_{i,M-1}W_{M-1,0}\]

&lt;p&gt;å¯è§ $S_i$ çš„ç¬¬ $k$ ä¸ªåˆ†é‡å…³äº $W$ çš„åå¯¼æ•°åªå’Œ $W$ çš„ç¬¬ $k$  åˆ—æœ‰å…³ï¼Œåˆ™ ï¼š&lt;/p&gt;

\[\frac{\partial{S_{i,0}}}{\partial{W_{0,0}}} = X_{i,0} \\
\frac{\partial{S_{i,0}}}{\partial{W_{1,0}}} = X_{i,1} \\
...\\
\frac{\partial{S_{i,0}}}{\partial{W_{M-1,0}}} = X_{i,M-1}\]

&lt;p&gt;é‚£ä¹ˆï¼š&lt;/p&gt;

\[\frac{\partial{S_{i,0}}}{\partial{W_{j,0}}} = {X_i}^T, \space j\in[0,M-1]\]

&lt;p&gt;åŒç†ï¼Œ$S_{i,1},â€¦,S_{i,C-1}$ å¯¹ $W$ çš„åå¯¼æ•°å‡ä¸º ${X_i}^T$ï¼Œé‚£ä¹ˆå¯ä»¥å¾—åˆ°ï¼š&lt;/p&gt;

\[\frac{\partial{S_i}}{\partial{W}} = \left[{X_i}^T, ..., {X_i}^T\right]\]

&lt;p&gt;å…±æœ‰ M è¡Œ C åˆ—ã€‚æ³¨æ„è¿™åªæ˜¯ä¸€ä¸ªæ ·æœ¬çš„å¾—åˆ† $S_i$ å…³äºå‚æ•°çŸ©é˜µ $W$ çš„åå¯¼æ•°ã€‚&lt;/p&gt;

&lt;h3 id=&quot;33-åå‘ä¼ æ’­&quot;&gt;3.3 åå‘ä¼ æ’­&lt;/h3&gt;

&lt;p&gt;åå‘ä¼ æ’­çš„ç›®æ ‡æ˜¯æ±‚ $\frac{\partial{L}}{\partial{W}}$ ï¼Œç„¶åç”¨å®ƒå¯¹å‚æ•° $W$ è¿›è¡Œæ›´æ–°(æ¢¯åº¦ä¸‹é™)ã€‚å¯¹äºå•ä¸ªæ ·æœ¬æœ‰ï¼š&lt;/p&gt;

\[\begin{align}
\frac{\partial{L_i}}{\partial{W}}
&amp;amp;= \frac{\partial{L_i}}{\partial{Si}} \times \frac{\partial{S_i}}{\partial{W}} \\
&amp;amp;= (P_i - y_i) \times \left[{X_i}^T, ..., {X_i}^T\right]
\end{align}\]

&lt;p&gt;$P_i - y_i$ çš„å¤§å°ä¸º 1 x Cï¼Œ$\left[{X_i}^T, â€¦, {X_i}^T\right]$ çš„å¤§å°ä¸º M x Cï¼Œå…ƒç´ å¯¹åº”ç›¸ä¹˜å¹¶è¿›è¡Œè¡Œæ‰©å±•(row broadcasting)ã€‚å¯¹ Loss å‡½æ•°æ±‚å…³äº $W$ çš„åå¯¼æœ‰ï¼š&lt;/p&gt;

\[\frac{\partial{L}}{\partial{W}} =
\frac{1}{N}\sum_{i=1}^{N}\frac{\partial{L_i}}{\partial{W}}
+ \lambda W\]

&lt;p&gt;åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ±‚å’Œé¡¹å¯ä»¥å†™ä¸ºçŸ©é˜µä¹˜æ³•çš„å½¢å¼ï¼š&lt;/p&gt;

\[\begin{align}
\sum_{i=1}^{N}\frac{\partial{L_i}}{\partial{W}}
&amp;amp;= X^T\frac{\partial{L}}{\partial{S}} \\
&amp;amp;= X^T(P-y)
\end{align}\]

&lt;p&gt;å¦‚æœæ¯”è¾ƒéš¾ç†è§£ï¼Œå¯ä»¥å°†è¿™ä¸ªçŸ©é˜µä¹˜æ³•å±•å¼€ï¼ˆä¸ºäº†æ–¹ä¾¿ï¼Œä¸‹æ ‡ä» 1 å¼€å§‹ï¼‰ï¼Œæˆ‘ä»¬è®° $D = P-y$ï¼ˆSoftmax ä¼ æ¥çš„æ¢¯åº¦çŸ©é˜µï¼‰ï¼š&lt;/p&gt;

\[\begin{align}
\sum_{i=1}^{N}\frac{\partial{L_i}}{\partial{W}}
&amp;amp;= X^TD \\
&amp;amp;=
\begin{bmatrix}
X_{11} &amp;amp; X_{21} &amp;amp; ... &amp;amp; X_{N1} \\
X_{12} &amp;amp; X_{22} &amp;amp; ... &amp;amp; X_{N2} \\
... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\
X_{1M} &amp;amp; X_{2M} &amp;amp; ... &amp;amp; X_{NM} \\
\end{bmatrix}
\begin{bmatrix}
D_{11} &amp;amp; D_{12} &amp;amp; ... &amp;amp; D_{1C} \\
D_{21} &amp;amp; D_{22} &amp;amp; ... &amp;amp; D_{2C} \\
... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\
D_{N1} &amp;amp; D_{N2} &amp;amp; ... &amp;amp; D_{NC} \\
\end{bmatrix} \\
&amp;amp;=
\begin{bmatrix}
dW_{11} &amp;amp; dW_{12} &amp;amp; ... &amp;amp; dW_{1C} \\
dW_{21} &amp;amp; dW_{22} &amp;amp; ... &amp;amp; dW_{2C} \\
... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\
dW_{M1} &amp;amp; dW_{M2} &amp;amp; ... &amp;amp; dW_{MC} \\
\end{bmatrix}
\end{align}\]

&lt;p&gt;ä»¥ $dW_{11}$ ä¸ºä¾‹å±•å¼€ï¼š&lt;/p&gt;

\[dW_{11} = X_{11}D_{11} + X_{21}D_{21} + ... + X_{N1}D_{N1}\]

&lt;p&gt;å³ $dW_{11}$ æ˜¯æ¯ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªåˆ†é‡å’Œå¯¹åº”ä¸Šæ¸¸åå¯¼æ•°çš„ä¹˜ç§¯ä¹‹å’Œï¼Œå³åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šåº”ç”¨é“¾å¼æ³•åˆ™å†æ±‚å’Œã€‚&lt;/p&gt;

&lt;h2 id=&quot;4-ç¤ºä¾‹ä»£ç &quot;&gt;4 ç¤ºä¾‹ä»£ç &lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Softmax loss function

    Inputs have dimension D, there are C classes, and we operate on minibatches
    of N examples.

    Inputs:
    - W: A numpy array of shape (D, C) containing weights.
    - X: A numpy array of shape (N, D) containing a minibatch of data.
    - y: A numpy array of shape (N,) containing training labels; y[i] = c means
      that X[i] has label c, where 0 &amp;lt;= c &amp;lt; C.
    - reg: (float) regularization strength

    Returns a tuple of:
    - loss as single float
    - gradient with respect to weights W; an array of same shape as W
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Initialize the loss and gradient to zero.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# calculate Loss
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# aviod numeric instability
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;row_sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row_sum&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# calculate dL/dW
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# store labels as one-hot vectors
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# shape: DxC
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# shape: DxN * NXC = D*C
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# regularization
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;5-å‚è€ƒæ–‡ç« &quot;&gt;5 å‚è€ƒæ–‡ç« &lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs231n.github.io/linear-classify/&quot;&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/&quot;&gt;The Softmax function and its derivative&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&quot;&gt;A Gentle Introduction to Cross-Entropy for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Tianxin Xie</name></author><category term="MachineLearning" /><category term="MachineLearning" /><summary type="html">Softmax å¸¸ç”¨äºå¤šåˆ†ç±»çš„æƒ…å†µï¼Œå®ƒå°†ä¸€ä¸ª N ç»´çš„ç”±ä»»æ„å®æ•°ç»„æˆçš„å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç„¶åè¾“å‡ºä¸€ä¸ª N ç»´çš„æ¦‚ç‡å‘é‡ï¼Œä¸”å…¶æ‰€æœ‰åˆ†é‡ä¹‹å’Œä¸º 1ã€‚æˆ‘ä»¬å…ˆæ¥çœ‹ä¸€ä¸ªæœ‰ 5 ç±»ï¼ˆç±»åˆ«ç¼–å·ä¸º 0, 1, 2, 3, 4ï¼‰è¾“å‡ºçš„å¤šåˆ†ç±»å™¨ï¼š</summary></entry></feed>